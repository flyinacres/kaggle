{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# XGB Optimization\n\nExperiment with XGB parameter optimization to see if it can be used with my Abalone experiments.\n\nhttps://medium.com/@attud_bidirt/automatic-tuning-of-hyper-parameters-of-a-xgboost-classifier-c5588bceda4\n\nNote that the code in this example does not work in the order specified.  The code blocks are misarranged, and there is some missing work.  But still interesting!","metadata":{}},{"cell_type":"markdown","source":"## Get data right","metadata":{}},{"cell_type":"code","source":"from sklearn import datasets\n\n# without the as frame this is loaded as Bunch\ndata = datasets.load_breast_cancer(as_frame=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T21:05:53.046452Z","iopub.execute_input":"2025-01-23T21:05:53.047020Z","iopub.status.idle":"2025-01-23T21:05:55.213033Z","shell.execute_reply.started":"2025-01-23T21:05:53.046988Z","shell.execute_reply":"2025-01-23T21:05:55.212028Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"type(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:36:56.757911Z","iopub.execute_input":"2025-01-22T08:36:56.758658Z","iopub.status.idle":"2025-01-22T08:36:56.764652Z","shell.execute_reply.started":"2025-01-22T08:36:56.758607Z","shell.execute_reply":"2025-01-22T08:36:56.763577Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"sklearn.utils._bunch.Bunch"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"data.frame","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:37:29.933467Z","iopub.execute_input":"2025-01-22T08:37:29.933803Z","iopub.status.idle":"2025-01-22T08:37:29.968911Z","shell.execute_reply.started":"2025-01-22T08:37:29.933778Z","shell.execute_reply":"2025-01-22T08:37:29.967564Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n0          17.99         10.38          122.80     1001.0          0.11840   \n1          20.57         17.77          132.90     1326.0          0.08474   \n2          19.69         21.25          130.00     1203.0          0.10960   \n3          11.42         20.38           77.58      386.1          0.14250   \n4          20.29         14.34          135.10     1297.0          0.10030   \n..           ...           ...             ...        ...              ...   \n564        21.56         22.39          142.00     1479.0          0.11100   \n565        20.13         28.25          131.20     1261.0          0.09780   \n566        16.60         28.08          108.30      858.1          0.08455   \n567        20.60         29.33          140.10     1265.0          0.11780   \n568         7.76         24.54           47.92      181.0          0.05263   \n\n     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n0             0.27760         0.30010              0.14710         0.2419   \n1             0.07864         0.08690              0.07017         0.1812   \n2             0.15990         0.19740              0.12790         0.2069   \n3             0.28390         0.24140              0.10520         0.2597   \n4             0.13280         0.19800              0.10430         0.1809   \n..                ...             ...                  ...            ...   \n564           0.11590         0.24390              0.13890         0.1726   \n565           0.10340         0.14400              0.09791         0.1752   \n566           0.10230         0.09251              0.05302         0.1590   \n567           0.27700         0.35140              0.15200         0.2397   \n568           0.04362         0.00000              0.00000         0.1587   \n\n     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n0                   0.07871  ...          17.33           184.60      2019.0   \n1                   0.05667  ...          23.41           158.80      1956.0   \n2                   0.05999  ...          25.53           152.50      1709.0   \n3                   0.09744  ...          26.50            98.87       567.7   \n4                   0.05883  ...          16.67           152.20      1575.0   \n..                      ...  ...            ...              ...         ...   \n564                 0.05623  ...          26.40           166.10      2027.0   \n565                 0.05533  ...          38.25           155.00      1731.0   \n566                 0.05648  ...          34.12           126.70      1124.0   \n567                 0.07016  ...          39.42           184.60      1821.0   \n568                 0.05884  ...          30.37            59.16       268.6   \n\n     worst smoothness  worst compactness  worst concavity  \\\n0             0.16220            0.66560           0.7119   \n1             0.12380            0.18660           0.2416   \n2             0.14440            0.42450           0.4504   \n3             0.20980            0.86630           0.6869   \n4             0.13740            0.20500           0.4000   \n..                ...                ...              ...   \n564           0.14100            0.21130           0.4107   \n565           0.11660            0.19220           0.3215   \n566           0.11390            0.30940           0.3403   \n567           0.16500            0.86810           0.9387   \n568           0.08996            0.06444           0.0000   \n\n     worst concave points  worst symmetry  worst fractal dimension  target  \n0                  0.2654          0.4601                  0.11890       0  \n1                  0.1860          0.2750                  0.08902       0  \n2                  0.2430          0.3613                  0.08758       0  \n3                  0.2575          0.6638                  0.17300       0  \n4                  0.1625          0.2364                  0.07678       0  \n..                    ...             ...                      ...     ...  \n564                0.2216          0.2060                  0.07115       0  \n565                0.1628          0.2572                  0.06637       0  \n566                0.1418          0.2218                  0.07820       0  \n567                0.2650          0.4087                  0.12400       0  \n568                0.0000          0.2871                  0.07039       1  \n\n[569 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean radius</th>\n      <th>mean texture</th>\n      <th>mean perimeter</th>\n      <th>mean area</th>\n      <th>mean smoothness</th>\n      <th>mean compactness</th>\n      <th>mean concavity</th>\n      <th>mean concave points</th>\n      <th>mean symmetry</th>\n      <th>mean fractal dimension</th>\n      <th>...</th>\n      <th>worst texture</th>\n      <th>worst perimeter</th>\n      <th>worst area</th>\n      <th>worst smoothness</th>\n      <th>worst compactness</th>\n      <th>worst concavity</th>\n      <th>worst concave points</th>\n      <th>worst symmetry</th>\n      <th>worst fractal dimension</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.30010</td>\n      <td>0.14710</td>\n      <td>0.2419</td>\n      <td>0.07871</td>\n      <td>...</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.16220</td>\n      <td>0.66560</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.08690</td>\n      <td>0.07017</td>\n      <td>0.1812</td>\n      <td>0.05667</td>\n      <td>...</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.12380</td>\n      <td>0.18660</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.19740</td>\n      <td>0.12790</td>\n      <td>0.2069</td>\n      <td>0.05999</td>\n      <td>...</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.14440</td>\n      <td>0.42450</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.24140</td>\n      <td>0.10520</td>\n      <td>0.2597</td>\n      <td>0.09744</td>\n      <td>...</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.20980</td>\n      <td>0.86630</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.19800</td>\n      <td>0.10430</td>\n      <td>0.1809</td>\n      <td>0.05883</td>\n      <td>...</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.13740</td>\n      <td>0.20500</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>564</th>\n      <td>21.56</td>\n      <td>22.39</td>\n      <td>142.00</td>\n      <td>1479.0</td>\n      <td>0.11100</td>\n      <td>0.11590</td>\n      <td>0.24390</td>\n      <td>0.13890</td>\n      <td>0.1726</td>\n      <td>0.05623</td>\n      <td>...</td>\n      <td>26.40</td>\n      <td>166.10</td>\n      <td>2027.0</td>\n      <td>0.14100</td>\n      <td>0.21130</td>\n      <td>0.4107</td>\n      <td>0.2216</td>\n      <td>0.2060</td>\n      <td>0.07115</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>565</th>\n      <td>20.13</td>\n      <td>28.25</td>\n      <td>131.20</td>\n      <td>1261.0</td>\n      <td>0.09780</td>\n      <td>0.10340</td>\n      <td>0.14400</td>\n      <td>0.09791</td>\n      <td>0.1752</td>\n      <td>0.05533</td>\n      <td>...</td>\n      <td>38.25</td>\n      <td>155.00</td>\n      <td>1731.0</td>\n      <td>0.11660</td>\n      <td>0.19220</td>\n      <td>0.3215</td>\n      <td>0.1628</td>\n      <td>0.2572</td>\n      <td>0.06637</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>566</th>\n      <td>16.60</td>\n      <td>28.08</td>\n      <td>108.30</td>\n      <td>858.1</td>\n      <td>0.08455</td>\n      <td>0.10230</td>\n      <td>0.09251</td>\n      <td>0.05302</td>\n      <td>0.1590</td>\n      <td>0.05648</td>\n      <td>...</td>\n      <td>34.12</td>\n      <td>126.70</td>\n      <td>1124.0</td>\n      <td>0.11390</td>\n      <td>0.30940</td>\n      <td>0.3403</td>\n      <td>0.1418</td>\n      <td>0.2218</td>\n      <td>0.07820</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>567</th>\n      <td>20.60</td>\n      <td>29.33</td>\n      <td>140.10</td>\n      <td>1265.0</td>\n      <td>0.11780</td>\n      <td>0.27700</td>\n      <td>0.35140</td>\n      <td>0.15200</td>\n      <td>0.2397</td>\n      <td>0.07016</td>\n      <td>...</td>\n      <td>39.42</td>\n      <td>184.60</td>\n      <td>1821.0</td>\n      <td>0.16500</td>\n      <td>0.86810</td>\n      <td>0.9387</td>\n      <td>0.2650</td>\n      <td>0.4087</td>\n      <td>0.12400</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>568</th>\n      <td>7.76</td>\n      <td>24.54</td>\n      <td>47.92</td>\n      <td>181.0</td>\n      <td>0.05263</td>\n      <td>0.04362</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.1587</td>\n      <td>0.05884</td>\n      <td>...</td>\n      <td>30.37</td>\n      <td>59.16</td>\n      <td>268.6</td>\n      <td>0.08996</td>\n      <td>0.06444</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.2871</td>\n      <td>0.07039</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>569 rows × 31 columns</p>\n</div>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"data.frame.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:37:57.312217Z","iopub.execute_input":"2025-01-22T08:37:57.312511Z","iopub.status.idle":"2025-01-22T08:37:57.318572Z","shell.execute_reply.started":"2025-01-22T08:37:57.312492Z","shell.execute_reply":"2025-01-22T08:37:57.317633Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"Index(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n       'mean smoothness', 'mean compactness', 'mean concavity',\n       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n       'radius error', 'texture error', 'perimeter error', 'area error',\n       'smoothness error', 'compactness error', 'concavity error',\n       'concave points error', 'symmetry error', 'fractal dimension error',\n       'worst radius', 'worst texture', 'worst perimeter', 'worst area',\n       'worst smoothness', 'worst compactness', 'worst concavity',\n       'worst concave points', 'worst symmetry', 'worst fractal dimension',\n       'target'],\n      dtype='object')"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"data.frame.target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:39:27.917817Z","iopub.execute_input":"2025-01-22T08:39:27.918182Z","iopub.status.idle":"2025-01-22T08:39:27.925945Z","shell.execute_reply.started":"2025-01-22T08:39:27.918158Z","shell.execute_reply":"2025-01-22T08:39:27.924120Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"0      0\n1      0\n2      0\n3      0\n4      0\n      ..\n564    0\n565    0\n566    0\n567    0\n568    1\nName: target, Length: 569, dtype: int64"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"# Need to separate data into X and Y\nX = data.frame.drop('target', axis = 1)\ny = data.frame.target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T21:06:09.332562Z","iopub.execute_input":"2025-01-23T21:06:09.332969Z","iopub.status.idle":"2025-01-23T21:06:09.339038Z","shell.execute_reply.started":"2025-01-23T21:06:09.332935Z","shell.execute_reply":"2025-01-23T21:06:09.338006Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:41:16.116811Z","iopub.execute_input":"2025-01-22T08:41:16.117187Z","iopub.status.idle":"2025-01-22T08:41:16.151503Z","shell.execute_reply.started":"2025-01-22T08:41:16.117156Z","shell.execute_reply":"2025-01-22T08:41:16.147890Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n0          17.99         10.38          122.80     1001.0          0.11840   \n1          20.57         17.77          132.90     1326.0          0.08474   \n2          19.69         21.25          130.00     1203.0          0.10960   \n3          11.42         20.38           77.58      386.1          0.14250   \n4          20.29         14.34          135.10     1297.0          0.10030   \n..           ...           ...             ...        ...              ...   \n564        21.56         22.39          142.00     1479.0          0.11100   \n565        20.13         28.25          131.20     1261.0          0.09780   \n566        16.60         28.08          108.30      858.1          0.08455   \n567        20.60         29.33          140.10     1265.0          0.11780   \n568         7.76         24.54           47.92      181.0          0.05263   \n\n     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n0             0.27760         0.30010              0.14710         0.2419   \n1             0.07864         0.08690              0.07017         0.1812   \n2             0.15990         0.19740              0.12790         0.2069   \n3             0.28390         0.24140              0.10520         0.2597   \n4             0.13280         0.19800              0.10430         0.1809   \n..                ...             ...                  ...            ...   \n564           0.11590         0.24390              0.13890         0.1726   \n565           0.10340         0.14400              0.09791         0.1752   \n566           0.10230         0.09251              0.05302         0.1590   \n567           0.27700         0.35140              0.15200         0.2397   \n568           0.04362         0.00000              0.00000         0.1587   \n\n     mean fractal dimension  ...  worst radius  worst texture  \\\n0                   0.07871  ...        25.380          17.33   \n1                   0.05667  ...        24.990          23.41   \n2                   0.05999  ...        23.570          25.53   \n3                   0.09744  ...        14.910          26.50   \n4                   0.05883  ...        22.540          16.67   \n..                      ...  ...           ...            ...   \n564                 0.05623  ...        25.450          26.40   \n565                 0.05533  ...        23.690          38.25   \n566                 0.05648  ...        18.980          34.12   \n567                 0.07016  ...        25.740          39.42   \n568                 0.05884  ...         9.456          30.37   \n\n     worst perimeter  worst area  worst smoothness  worst compactness  \\\n0             184.60      2019.0           0.16220            0.66560   \n1             158.80      1956.0           0.12380            0.18660   \n2             152.50      1709.0           0.14440            0.42450   \n3              98.87       567.7           0.20980            0.86630   \n4             152.20      1575.0           0.13740            0.20500   \n..               ...         ...               ...                ...   \n564           166.10      2027.0           0.14100            0.21130   \n565           155.00      1731.0           0.11660            0.19220   \n566           126.70      1124.0           0.11390            0.30940   \n567           184.60      1821.0           0.16500            0.86810   \n568            59.16       268.6           0.08996            0.06444   \n\n     worst concavity  worst concave points  worst symmetry  \\\n0             0.7119                0.2654          0.4601   \n1             0.2416                0.1860          0.2750   \n2             0.4504                0.2430          0.3613   \n3             0.6869                0.2575          0.6638   \n4             0.4000                0.1625          0.2364   \n..               ...                   ...             ...   \n564           0.4107                0.2216          0.2060   \n565           0.3215                0.1628          0.2572   \n566           0.3403                0.1418          0.2218   \n567           0.9387                0.2650          0.4087   \n568           0.0000                0.0000          0.2871   \n\n     worst fractal dimension  \n0                    0.11890  \n1                    0.08902  \n2                    0.08758  \n3                    0.17300  \n4                    0.07678  \n..                       ...  \n564                  0.07115  \n565                  0.06637  \n566                  0.07820  \n567                  0.12400  \n568                  0.07039  \n\n[569 rows x 30 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean radius</th>\n      <th>mean texture</th>\n      <th>mean perimeter</th>\n      <th>mean area</th>\n      <th>mean smoothness</th>\n      <th>mean compactness</th>\n      <th>mean concavity</th>\n      <th>mean concave points</th>\n      <th>mean symmetry</th>\n      <th>mean fractal dimension</th>\n      <th>...</th>\n      <th>worst radius</th>\n      <th>worst texture</th>\n      <th>worst perimeter</th>\n      <th>worst area</th>\n      <th>worst smoothness</th>\n      <th>worst compactness</th>\n      <th>worst concavity</th>\n      <th>worst concave points</th>\n      <th>worst symmetry</th>\n      <th>worst fractal dimension</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.30010</td>\n      <td>0.14710</td>\n      <td>0.2419</td>\n      <td>0.07871</td>\n      <td>...</td>\n      <td>25.380</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.16220</td>\n      <td>0.66560</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.08690</td>\n      <td>0.07017</td>\n      <td>0.1812</td>\n      <td>0.05667</td>\n      <td>...</td>\n      <td>24.990</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.12380</td>\n      <td>0.18660</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.19740</td>\n      <td>0.12790</td>\n      <td>0.2069</td>\n      <td>0.05999</td>\n      <td>...</td>\n      <td>23.570</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.14440</td>\n      <td>0.42450</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.24140</td>\n      <td>0.10520</td>\n      <td>0.2597</td>\n      <td>0.09744</td>\n      <td>...</td>\n      <td>14.910</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.20980</td>\n      <td>0.86630</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.19800</td>\n      <td>0.10430</td>\n      <td>0.1809</td>\n      <td>0.05883</td>\n      <td>...</td>\n      <td>22.540</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.13740</td>\n      <td>0.20500</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>564</th>\n      <td>21.56</td>\n      <td>22.39</td>\n      <td>142.00</td>\n      <td>1479.0</td>\n      <td>0.11100</td>\n      <td>0.11590</td>\n      <td>0.24390</td>\n      <td>0.13890</td>\n      <td>0.1726</td>\n      <td>0.05623</td>\n      <td>...</td>\n      <td>25.450</td>\n      <td>26.40</td>\n      <td>166.10</td>\n      <td>2027.0</td>\n      <td>0.14100</td>\n      <td>0.21130</td>\n      <td>0.4107</td>\n      <td>0.2216</td>\n      <td>0.2060</td>\n      <td>0.07115</td>\n    </tr>\n    <tr>\n      <th>565</th>\n      <td>20.13</td>\n      <td>28.25</td>\n      <td>131.20</td>\n      <td>1261.0</td>\n      <td>0.09780</td>\n      <td>0.10340</td>\n      <td>0.14400</td>\n      <td>0.09791</td>\n      <td>0.1752</td>\n      <td>0.05533</td>\n      <td>...</td>\n      <td>23.690</td>\n      <td>38.25</td>\n      <td>155.00</td>\n      <td>1731.0</td>\n      <td>0.11660</td>\n      <td>0.19220</td>\n      <td>0.3215</td>\n      <td>0.1628</td>\n      <td>0.2572</td>\n      <td>0.06637</td>\n    </tr>\n    <tr>\n      <th>566</th>\n      <td>16.60</td>\n      <td>28.08</td>\n      <td>108.30</td>\n      <td>858.1</td>\n      <td>0.08455</td>\n      <td>0.10230</td>\n      <td>0.09251</td>\n      <td>0.05302</td>\n      <td>0.1590</td>\n      <td>0.05648</td>\n      <td>...</td>\n      <td>18.980</td>\n      <td>34.12</td>\n      <td>126.70</td>\n      <td>1124.0</td>\n      <td>0.11390</td>\n      <td>0.30940</td>\n      <td>0.3403</td>\n      <td>0.1418</td>\n      <td>0.2218</td>\n      <td>0.07820</td>\n    </tr>\n    <tr>\n      <th>567</th>\n      <td>20.60</td>\n      <td>29.33</td>\n      <td>140.10</td>\n      <td>1265.0</td>\n      <td>0.11780</td>\n      <td>0.27700</td>\n      <td>0.35140</td>\n      <td>0.15200</td>\n      <td>0.2397</td>\n      <td>0.07016</td>\n      <td>...</td>\n      <td>25.740</td>\n      <td>39.42</td>\n      <td>184.60</td>\n      <td>1821.0</td>\n      <td>0.16500</td>\n      <td>0.86810</td>\n      <td>0.9387</td>\n      <td>0.2650</td>\n      <td>0.4087</td>\n      <td>0.12400</td>\n    </tr>\n    <tr>\n      <th>568</th>\n      <td>7.76</td>\n      <td>24.54</td>\n      <td>47.92</td>\n      <td>181.0</td>\n      <td>0.05263</td>\n      <td>0.04362</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.1587</td>\n      <td>0.05884</td>\n      <td>...</td>\n      <td>9.456</td>\n      <td>30.37</td>\n      <td>59.16</td>\n      <td>268.6</td>\n      <td>0.08996</td>\n      <td>0.06444</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.2871</td>\n      <td>0.07039</td>\n    </tr>\n  </tbody>\n</table>\n<p>569 rows × 30 columns</p>\n</div>"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:41:27.995161Z","iopub.execute_input":"2025-01-22T08:41:27.995451Z","iopub.status.idle":"2025-01-22T08:41:28.003148Z","shell.execute_reply.started":"2025-01-22T08:41:27.995433Z","shell.execute_reply":"2025-01-22T08:41:28.001707Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"0      0\n1      0\n2      0\n3      0\n4      0\n      ..\n564    0\n565    0\n566    0\n567    0\n568    1\nName: target, Length: 569, dtype: int64"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"from pandas.core.common import random_state\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nss = StratifiedShuffleSplit(2,test_size=0.2, random_state=44)\nfor tr_idx,ts_idx in ss.split(X,y):\n  X_train, y_train = X.loc[tr_idx], y.loc[tr_idx]\n  X_test, y_test = X.loc[ts_idx], y.loc[ts_idx]\n\nprint(f\"\\nShape of X_train is {X_train.shape}\")\nprint(f\"\\nShape of X_test is {X_test.shape}\")\nprint(f\"\\nLength of y_train is {y_train.shape}\")\nprint(f\"\\nLength of y_test is {y_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T21:06:18.678094Z","iopub.execute_input":"2025-01-23T21:06:18.678437Z","iopub.status.idle":"2025-01-23T21:06:18.774844Z","shell.execute_reply.started":"2025-01-23T21:06:18.678403Z","shell.execute_reply":"2025-01-23T21:06:18.774131Z"}},"outputs":[{"name":"stdout","text":"\nShape of X_train is (455, 30)\n\nShape of X_test is (114, 30)\n\nLength of y_train is (455,)\n\nLength of y_test is (114,)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Preliminary Tests","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb \nfrom sklearn.model_selection import cross_val_score \nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe, space_eval \nfrom hyperopt.early_stop import no_progress_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:42:32.140627Z","iopub.execute_input":"2025-01-22T08:42:32.141014Z","iopub.status.idle":"2025-01-22T08:42:33.763498Z","shell.execute_reply.started":"2025-01-22T08:42:32.140982Z","shell.execute_reply":"2025-01-22T08:42:33.762368Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"dtrain_clf = xgb.DMatrix(X_train, y_train, enable_categorical = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:43:26.865849Z","iopub.execute_input":"2025-01-22T08:43:26.866229Z","iopub.status.idle":"2025-01-22T08:43:26.904912Z","shell.execute_reply.started":"2025-01-22T08:43:26.866210Z","shell.execute_reply":"2025-01-22T08:43:26.903359Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"from sklearn.metrics import f1_score, recall_score, confusion_matrix,roc_auc_score\n\nparams_1 = {\"objective\": \"binary:logistic\"}\n\nn = 1000\n\nresults = xgb.cv(params_1,\n                 dtrain_clf,\n                 num_boost_round = n,\n                 nfold=5,\n                 metrics = [\"logloss\",\"auc\",\"error\"],\n                 early_stopping_rounds=20\n                 )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:43:29.674751Z","iopub.execute_input":"2025-01-22T08:43:29.675077Z","iopub.status.idle":"2025-01-22T08:43:30.235628Z","shell.execute_reply.started":"2025-01-22T08:43:29.675058Z","shell.execute_reply":"2025-01-22T08:43:30.234949Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"clf_1 = xgb.XGBClassifier(**params_1)\n\nclf_1.fit(X_train,y_train)\n\npred_1 = clf_1.predict(X_test)\n\nprint(f\"f1 score : {f1_score(y_test, pred_1)}\\n\")\nprint(f\"confusion Matrix:\\n{confusion_matrix(y_test, pred_1)}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:44:37.400611Z","iopub.execute_input":"2025-01-22T08:44:37.400910Z","iopub.status.idle":"2025-01-22T08:44:37.492274Z","shell.execute_reply.started":"2025-01-22T08:44:37.400881Z","shell.execute_reply":"2025-01-22T08:44:37.491225Z"}},"outputs":[{"name":"stdout","text":"f1 score : 0.9403973509933775\n\nconfusion Matrix:\n[[34  8]\n [ 1 71]]\n\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"## Now Hyperopt","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb \nfrom sklearn.model_selection import cross_val_score \nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe, space_eval \nfrom hyperopt.early_stop import no_progress_loss\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:47:46.097651Z","iopub.execute_input":"2025-01-22T08:47:46.097978Z","iopub.status.idle":"2025-01-22T08:47:46.102675Z","shell.execute_reply.started":"2025-01-22T08:47:46.097953Z","shell.execute_reply":"2025-01-22T08:47:46.101335Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"search_space = {\n    'max_depth': hp.choice(\"max_depth\", np.arange(1,20,1,dtype=int)),\n    'eta'      : hp.uniform(\"eta\", 0, 1),\n    'gamma'    : hp.uniform(\"gamma\", 0, 10e1),\n    'reg_alpha': hp.uniform(\"reg_alpha\", 10e-7, 10),\n    'reg_lambda' : hp.uniform(\"reg_lambda\", 0,1),\n    'colsample_bytree': hp.uniform(\"colsample_bytree\", 0.5,1),\n    'colsample_bynode': hp.uniform(\"colsample_bynode\", 0.5,1), \n    'colsample_bylevel': hp.uniform(\"colsample_bylevel\", 0.5,1),\n    'n_estimators': hp.choice(\"n_estimators\", np.arange(100,1000,10,dtype='int')),\n    'min_child_weight' : hp.choice(\"min_child_weight\", np.arange(1,10,1,dtype='int')),\n    'max_delta_step' : hp.choice(\"max_delta_step\", np.arange(1,10,1,dtype='int')),\n    'subsample' : hp.uniform(\"subsample\",0.5,1),\n    'objective' : 'binary:logistic',\n    'eval_metric' : 'aucpr',\n    'seed' : 44\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:47:49.081511Z","iopub.execute_input":"2025-01-22T08:47:49.081857Z","iopub.status.idle":"2025-01-22T08:47:49.088999Z","shell.execute_reply.started":"2025-01-22T08:47:49.081828Z","shell.execute_reply":"2025-01-22T08:47:49.087345Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"def xgb_objective(space):\n  results = xgb.cv(space, \n                   dtrain=dtrain_clf, #DMatrix (xgboost specific)\n                   num_boost_round=500, \n                   nfold=5, \n                   stratified=True,  \n                   early_stopping_rounds=20,\n                   metrics = ['logloss','auc','aucpr','error'])\n  \n  best_score = results['test-auc-mean'].max()\n  return {'loss':-best_score, 'status': STATUS_OK}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:48:06.987544Z","iopub.execute_input":"2025-01-22T08:48:06.987886Z","iopub.status.idle":"2025-01-22T08:48:06.994694Z","shell.execute_reply.started":"2025-01-22T08:48:06.987867Z","shell.execute_reply":"2025-01-22T08:48:06.991559Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"trials = Trials()\n\nbest_hyperparams = fmin(fn=xgb_objective, space=search_space,algo=tpe.suggest,max_evals=500,trials=trials, return_argmin=False, early_stop_fn=no_progress_loss(10))\n\nbest_params = best_hyperparams.copy()\n\n# `eval_metric` is a key that is not a hyperparameter of the classifier\nif 'eval_metric' in best_params:\n  best_params = {key:best_params[key] for key in best_params if key!='eval_metric'}\n\nbest_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:48:23.242795Z","iopub.execute_input":"2025-01-22T08:48:23.243160Z","iopub.status.idle":"2025-01-22T08:48:27.957421Z","shell.execute_reply.started":"2025-01-22T08:48:23.243133Z","shell.execute_reply":"2025-01-22T08:48:27.956311Z"}},"outputs":[{"name":"stdout","text":"  0%|          | 1/500 [00:00<00:54,  9.16trial/s, best loss: -0.8919504643962849]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:23] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:23] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  1%|          | 3/500 [00:00<01:02,  7.93trial/s, best loss: -0.9722910216718266]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:23] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:23] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  1%|          | 5/500 [00:00<01:08,  7.26trial/s, best loss: -0.972703818369453] ","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:23] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:23] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  1%|▏         | 7/500 [00:00<01:04,  7.69trial/s, best loss: -0.972703818369453]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:24] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:24] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  2%|▏         | 9/500 [00:01<01:07,  7.32trial/s, best loss: -0.9824561403508772]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:24] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:24] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  2%|▏         | 11/500 [00:01<01:27,  5.56trial/s, best loss: -0.986532507739938] ","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:24] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:24] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  2%|▏         | 12/500 [00:01<01:18,  6.22trial/s, best loss: -0.986532507739938]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:25] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  3%|▎         | 13/500 [00:01<01:28,  5.53trial/s, best loss: -0.986532507739938]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:25] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  3%|▎         | 15/500 [00:02<01:24,  5.75trial/s, best loss: -0.9919504643962849]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:25] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:25] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  3%|▎         | 17/500 [00:02<01:11,  6.72trial/s, best loss: -0.9919504643962849]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:25] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:26] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  4%|▍         | 19/500 [00:03<02:49,  2.85trial/s, best loss: -0.9919504643962849]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:27] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:27] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  4%|▍         | 21/500 [00:04<02:02,  3.91trial/s, best loss: -0.9919504643962849]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:27] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:27] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  5%|▍         | 23/500 [00:04<01:39,  4.81trial/s, best loss: -0.9919504643962849]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:27] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:27] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  5%|▍         | 24/500 [00:04<01:32,  5.13trial/s, best loss: -0.9919504643962849]\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"{'colsample_bylevel': 0.7370830882900052,\n 'colsample_bynode': 0.9456419018900569,\n 'colsample_bytree': 0.6886367856734236,\n 'eta': 0.5586879476999288,\n 'gamma': 1.9552221486075605,\n 'max_delta_step': 4,\n 'max_depth': 9,\n 'min_child_weight': 5,\n 'n_estimators': 630,\n 'objective': 'binary:logistic',\n 'reg_alpha': 0.939894791803691,\n 'reg_lambda': 0.5277342478394749,\n 'seed': 44,\n 'subsample': 0.5568365639214825}"},"metadata":{}}],"execution_count":37},{"cell_type":"markdown","source":"## Test with optimized params\n\nResults show that the 'best' params are meh. It may well have overfit","metadata":{}},{"cell_type":"code","source":"clf_2 = xgb.XGBClassifier(**best_params)\nclf_2.fit(X_train, y_train)\n\npred_2 = clf_2.predict(X_test)\n\nprint(f\"f1 score : {f1_score(y_test, pred_2)}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:49:23.123259Z","iopub.execute_input":"2025-01-22T08:49:23.123596Z","iopub.status.idle":"2025-01-22T08:49:23.316392Z","shell.execute_reply.started":"2025-01-22T08:49:23.123573Z","shell.execute_reply":"2025-01-22T08:49:23.314875Z"}},"outputs":[{"name":"stdout","text":"f1 score : 0.9459459459459458\n\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"search_space_3 = {\n    \n    'eta'      : hp.uniform(\"eta\", 0, 1),\n    'gamma'    : 0,#hp.uniform(\"gamma\", 0, 1),\n    'reg_lambda' : hp.uniform(\"reg_lambda\", 0,1),\n    'n_estimators': hp.choice(\"n_estimators\", np.arange(100,1000,10,dtype='int')),\n    'objective' : 'binary:logistic',\n    'eval_metric' : 'auc',\n    'seed' : 44\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:52:39.320038Z","iopub.execute_input":"2025-01-22T08:52:39.320421Z","iopub.status.idle":"2025-01-22T08:52:39.327110Z","shell.execute_reply.started":"2025-01-22T08:52:39.320400Z","shell.execute_reply":"2025-01-22T08:52:39.325741Z"}},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":"Hand tuned search space...","metadata":{}},{"cell_type":"code","source":"trials = Trials()\n\nbest_hyperparams = fmin(fn=xgb_objective, space=search_space_3,algo=tpe.suggest,max_evals=500,trials=trials, return_argmin=False, early_stop_fn=no_progress_loss(10))\n\nbest_params = best_hyperparams.copy()\n\n# `eval_metric` is a key that is not a hyperparameter of the classifier\nif 'eval_metric' in best_params:\n  best_params = {key:best_params[key] for key in best_params if key!='eval_metric'}\n\nbest_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:53:02.962941Z","iopub.execute_input":"2025-01-22T08:53:02.963379Z","iopub.status.idle":"2025-01-22T08:53:09.813165Z","shell.execute_reply.started":"2025-01-22T08:53:02.963345Z","shell.execute_reply":"2025-01-22T08:53:09.811836Z"}},"outputs":[{"name":"stdout","text":"  0%|          | 0/500 [00:00<?, ?trial/s, best loss=?]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:02] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:03] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  0%|          | 1/500 [00:00<01:57,  4.24trial/s, best loss: -0.9851909184726523]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:03] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  1%|          | 3/500 [00:01<02:50,  2.91trial/s, best loss: -0.9876160990712075]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:03] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:04] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  1%|          | 5/500 [00:01<02:04,  3.98trial/s, best loss: -0.9885964912280703]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:04] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:04] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  1%|▏         | 7/500 [00:01<02:00,  4.10trial/s, best loss: -0.9894736842105264]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:04] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:04] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  2%|▏         | 8/500 [00:02<02:06,  3.88trial/s, best loss: -0.9896284829721363]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:05] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  2%|▏         | 9/500 [00:02<02:03,  3.97trial/s, best loss: -0.9896284829721363]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:05] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  2%|▏         | 11/500 [00:02<01:49,  4.48trial/s, best loss: -0.9902476780185758]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:05] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:05] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  2%|▏         | 12/500 [00:03<02:12,  3.68trial/s, best loss: -0.9902476780185758]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:06] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  3%|▎         | 13/500 [00:03<02:08,  3.78trial/s, best loss: -0.9902476780185758]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:06] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  3%|▎         | 14/500 [00:03<02:06,  3.83trial/s, best loss: -0.9902476780185758]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:06] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  3%|▎         | 16/500 [00:04<02:11,  3.69trial/s, best loss: -0.9902476780185758]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:07] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:07] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  3%|▎         | 17/500 [00:04<02:20,  3.44trial/s, best loss: -0.9902476780185758]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:07] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  4%|▎         | 18/500 [00:04<02:12,  3.63trial/s, best loss: -0.9902476780185758]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:07] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  4%|▍         | 19/500 [00:06<05:09,  1.56trial/s, best loss: -0.9902476780185758]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:09] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  4%|▍         | 20/500 [00:06<02:43,  2.93trial/s, best loss: -0.9902476780185758]\n","output_type":"stream"},{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"{'eta': 0.6783655045838408,\n 'gamma': 0,\n 'n_estimators': 890,\n 'objective': 'binary:logistic',\n 'reg_lambda': 0.3896554989724714,\n 'seed': 44}"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"clf_2 = xgb.XGBClassifier(**best_params)\nclf_2.fit(X_train, y_train)\n\npred_2 = clf_2.predict(X_test)\n\nprint(f\"f1 score : {f1_score(y_test, pred_2)}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:53:33.915319Z","iopub.execute_input":"2025-01-22T08:53:33.915651Z","iopub.status.idle":"2025-01-22T08:53:34.862227Z","shell.execute_reply.started":"2025-01-22T08:53:33.915624Z","shell.execute_reply":"2025-01-22T08:53:34.861445Z"}},"outputs":[{"name":"stdout","text":"f1 score : 0.953020134228188\n\n","output_type":"stream"}],"execution_count":46},{"cell_type":"markdown","source":"## TODO\n\nThis article is interesting buy almost disproves itselt, as the search for better params didn't lead to much.  I should play with hyperopt a bit more to see if I can figure out how to get it to add value.","metadata":{}},{"cell_type":"markdown","source":"## Take 2\n\nhttps://medium.com/@justin.wesley.johns/precision-ml-engineering-with-xgboost-hyperopt-attaining-98-11-accuracy-on-mnist-d737b7ef1081\n\n","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\nfrom hyperopt import hp, fmin, tpe, STATUS_OK, Trials\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.datasets import mnist\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T21:06:42.422247Z","iopub.execute_input":"2025-01-23T21:06:42.422660Z","iopub.status.idle":"2025-01-23T21:06:55.154242Z","shell.execute_reply.started":"2025-01-23T21:06:42.422630Z","shell.execute_reply":"2025-01-23T21:06:55.153323Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Define the hyperparameter space\nspace = {\n    'max_depth': hp.quniform('max_depth', 3, 10, 1),\n    'gamma': hp.uniform('gamma', 0, 0.5),\n    'reg_alpha': hp.uniform('reg_alpha', 0, 0.1),\n    'reg_lambda': hp.uniform('reg_lambda', 0.5, 2),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n    'subsample': hp.uniform('subsample', 0.5, 1),\n    'n_estimators': hp.quniform('n_estimators', 100, 600, 50),\n    'learning_rate': hp.uniform('learning_rate', 0.05, 0.2),\n    'tree_method': 'gpu_hist'  # Use GPU\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T21:07:00.024092Z","iopub.execute_input":"2025-01-23T21:07:00.024720Z","iopub.status.idle":"2025-01-23T21:07:00.030059Z","shell.execute_reply.started":"2025-01-23T21:07:00.024689Z","shell.execute_reply":"2025-01-23T21:07:00.029212Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\n# Define the objective function\ndef objective(space):\n    clf = xgb.XGBClassifier(\n        #n_estimators=int(space['n_estimators']),\n        #max_depth=int(space['max_depth']),\n        gamma=space['gamma'],\n        reg_alpha=space['reg_alpha'],\n        reg_lambda=space['reg_lambda'],\n        colsample_bytree=space['colsample_bytree'],\n        #subsample=space['subsample'],\n        #learning_rate=space['learning_rate'],\n        #tree_method=space['tree_method'],\n        #use_label_encoder=False,  # Avoid the deprecation warning\n        #eval_metric='mlogloss'\n    )\n    evaluation = [(X_train, y_train), (X_test, y_test)]\n        \n    clf.fit(X_train, y_train,\n                eval_set=evaluation, verbose=True)\n        \n    pred = clf.predict(X_test)\n    accuracy = accuracy_score(y_test, pred)\n    print(f\"SCORE: {accuracy}\")\n\n    return {'loss': -accuracy, 'status': STATUS_OK}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T21:11:17.962988Z","iopub.execute_input":"2025-01-23T21:11:17.963336Z","iopub.status.idle":"2025-01-23T21:11:17.968180Z","shell.execute_reply.started":"2025-01-23T21:11:17.963304Z","shell.execute_reply":"2025-01-23T21:11:17.967293Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Run the optimization\ntrials = Trials()\nbest_hyperparams = fmin(fn=objective,\n                        space=space,\n                        algo=tpe.suggest,\n                        max_evals=200,\n                        trials=trials)\nprint(\"The best hyperparameters are: \", \"\\n\")\nprint(best_hyperparams)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T21:11:27.912689Z","iopub.execute_input":"2025-01-23T21:11:27.912997Z"}},"outputs":[{"name":"stdout","text":"[0]\tvalidation_0-logloss:0.43922\tvalidation_1-logloss:0.45901\n[1]\tvalidation_0-logloss:0.31471\tvalidation_1-logloss:0.35889\n[2]\tvalidation_0-logloss:0.23447\tvalidation_1-logloss:0.29498\n[3]\tvalidation_0-logloss:0.17822\tvalidation_1-logloss:0.25126\n[4]\tvalidation_0-logloss:0.13910\tvalidation_1-logloss:0.22440\n[5]\tvalidation_0-logloss:0.10948\tvalidation_1-logloss:0.20425\n[6]\tvalidation_0-logloss:0.08750\tvalidation_1-logloss:0.19383\n[7]\tvalidation_0-logloss:0.07265\tvalidation_1-logloss:0.18639\n[8]\tvalidation_0-logloss:0.06119\tvalidation_1-logloss:0.18305\n[9]\tvalidation_0-logloss:0.05192\tvalidation_1-logloss:0.18090\n[10]\tvalidation_0-logloss:0.04525\tvalidation_1-logloss:0.17862\n[11]\tvalidation_0-logloss:0.03948\tvalidation_1-logloss:0.17479\n[12]\tvalidation_0-logloss:0.03477\tvalidation_1-logloss:0.17059\n[13]\tvalidation_0-logloss:0.03129\tvalidation_1-logloss:0.16902\n[14]\tvalidation_0-logloss:0.02811\tvalidation_1-logloss:0.16583\n[15]\tvalidation_0-logloss:0.02580\tvalidation_1-logloss:0.16805\n[16]\tvalidation_0-logloss:0.02390\tvalidation_1-logloss:0.16858\n[17]\tvalidation_0-logloss:0.02172\tvalidation_1-logloss:0.16502\n[18]\tvalidation_0-logloss:0.02025\tvalidation_1-logloss:0.16554\n[19]\tvalidation_0-logloss:0.01899\tvalidation_1-logloss:0.16387\n[20]\tvalidation_0-logloss:0.01790\tvalidation_1-logloss:0.15889\n[21]\tvalidation_0-logloss:0.01701\tvalidation_1-logloss:0.16198\n[22]\tvalidation_0-logloss:0.01586\tvalidation_1-logloss:0.16044\n[23]\tvalidation_0-logloss:0.01503\tvalidation_1-logloss:0.16046\n[24]\tvalidation_0-logloss:0.01443\tvalidation_1-logloss:0.16043\n[25]\tvalidation_0-logloss:0.01414\tvalidation_1-logloss:0.16089\n[26]\tvalidation_0-logloss:0.01366\tvalidation_1-logloss:0.16235\n[27]\tvalidation_0-logloss:0.01340\tvalidation_1-logloss:0.16033\n[28]\tvalidation_0-logloss:0.01318\tvalidation_1-logloss:0.15904\n[29]\tvalidation_0-logloss:0.01275\tvalidation_1-logloss:0.15979\n[30]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15881\n[31]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15893\n[32]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15903\n[33]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15910\n[34]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15916\n[35]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15920\n[36]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15924\n[37]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15926\n[38]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15928\n[39]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15930\n[40]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15931\n[41]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15932\n[42]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15932\n[43]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15933\n[44]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15933\n[45]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15934\n[46]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15934\n[47]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15934\n[48]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15934\n[49]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15934\n[50]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15934\n[51]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15934\n[52]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15934\n[53]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15934\n[54]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15934\n[55]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[56]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[57]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[58]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[59]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[60]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[61]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[62]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[63]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[64]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[65]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[66]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[67]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[68]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[69]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[70]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[71]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[72]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[73]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[74]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[75]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[76]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[77]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[78]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[79]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[80]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[81]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[82]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[83]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[84]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[85]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[86]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[87]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[88]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[89]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[90]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[91]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[92]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[93]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[94]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[95]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[96]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[97]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[98]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\n[99]\tvalidation_0-logloss:0.01254\tvalidation_1-logloss:0.15935\nSCORE: 0.9122807017543859                              \n[0]\tvalidation_0-logloss:0.43471\tvalidation_1-logloss:0.45639                     \n[1]\tvalidation_0-logloss:0.30942\tvalidation_1-logloss:0.35518                     \n[2]\tvalidation_0-logloss:0.22888\tvalidation_1-logloss:0.29098                     \n[3]\tvalidation_0-logloss:0.17211\tvalidation_1-logloss:0.24631                     \n[4]\tvalidation_0-logloss:0.13248\tvalidation_1-logloss:0.22229                     \n[5]\tvalidation_0-logloss:0.10262\tvalidation_1-logloss:0.20447                     \n[6]\tvalidation_0-logloss:0.08133\tvalidation_1-logloss:0.19152                     \n[7]\tvalidation_0-logloss:0.06655\tvalidation_1-logloss:0.18217                     \n[8]\tvalidation_0-logloss:0.05530\tvalidation_1-logloss:0.18000                     \n[9]\tvalidation_0-logloss:0.04687\tvalidation_1-logloss:0.17498                     \n[10]\tvalidation_0-logloss:0.04037\tvalidation_1-logloss:0.17433                    \n[11]\tvalidation_0-logloss:0.03511\tvalidation_1-logloss:0.17173                    \n[12]\tvalidation_0-logloss:0.03046\tvalidation_1-logloss:0.16887                    \n[13]\tvalidation_0-logloss:0.02717\tvalidation_1-logloss:0.16954                    \n[14]\tvalidation_0-logloss:0.02452\tvalidation_1-logloss:0.17301                    \n[15]\tvalidation_0-logloss:0.02179\tvalidation_1-logloss:0.17333                    \n[16]\tvalidation_0-logloss:0.02006\tvalidation_1-logloss:0.17236                    \n[17]\tvalidation_0-logloss:0.01829\tvalidation_1-logloss:0.16806                    \n[18]\tvalidation_0-logloss:0.01695\tvalidation_1-logloss:0.16832                    \n[19]\tvalidation_0-logloss:0.01569\tvalidation_1-logloss:0.16933                    \n[20]\tvalidation_0-logloss:0.01482\tvalidation_1-logloss:0.16943                    \n[21]\tvalidation_0-logloss:0.01366\tvalidation_1-logloss:0.16622                    \n[22]\tvalidation_0-logloss:0.01279\tvalidation_1-logloss:0.16584                    \n[23]\tvalidation_0-logloss:0.01214\tvalidation_1-logloss:0.16471                    \n[24]\tvalidation_0-logloss:0.01123\tvalidation_1-logloss:0.16294                    \n[25]\tvalidation_0-logloss:0.01080\tvalidation_1-logloss:0.16256                    \n[26]\tvalidation_0-logloss:0.01052\tvalidation_1-logloss:0.16137                    \n[27]\tvalidation_0-logloss:0.01020\tvalidation_1-logloss:0.16160                    \n[28]\tvalidation_0-logloss:0.00991\tvalidation_1-logloss:0.15987                    \n[29]\tvalidation_0-logloss:0.00963\tvalidation_1-logloss:0.16070                    \n[30]\tvalidation_0-logloss:0.00932\tvalidation_1-logloss:0.16171                    \n[31]\tvalidation_0-logloss:0.00896\tvalidation_1-logloss:0.16536                    \n[32]\tvalidation_0-logloss:0.00871\tvalidation_1-logloss:0.16289                    \n[33]\tvalidation_0-logloss:0.00853\tvalidation_1-logloss:0.16323                    \n[34]\tvalidation_0-logloss:0.00832\tvalidation_1-logloss:0.16309                    \n[35]\tvalidation_0-logloss:0.00815\tvalidation_1-logloss:0.16190                    \n[36]\tvalidation_0-logloss:0.00805\tvalidation_1-logloss:0.16247                    \n[37]\tvalidation_0-logloss:0.00787\tvalidation_1-logloss:0.16270                    \n[38]\tvalidation_0-logloss:0.00762\tvalidation_1-logloss:0.16257                    \n[39]\tvalidation_0-logloss:0.00749\tvalidation_1-logloss:0.16276                    \n[40]\tvalidation_0-logloss:0.00740\tvalidation_1-logloss:0.16139                    \n[41]\tvalidation_0-logloss:0.00734\tvalidation_1-logloss:0.16088                    \n[42]\tvalidation_0-logloss:0.00728\tvalidation_1-logloss:0.16027                    \n[43]\tvalidation_0-logloss:0.00721\tvalidation_1-logloss:0.16044                    \n[44]\tvalidation_0-logloss:0.00715\tvalidation_1-logloss:0.16115                    \n[45]\tvalidation_0-logloss:0.00708\tvalidation_1-logloss:0.16000                    \n[46]\tvalidation_0-logloss:0.00703\tvalidation_1-logloss:0.16081                    \n[47]\tvalidation_0-logloss:0.00697\tvalidation_1-logloss:0.16154                    \n[48]\tvalidation_0-logloss:0.00691\tvalidation_1-logloss:0.16218                    \n[49]\tvalidation_0-logloss:0.00685\tvalidation_1-logloss:0.16262                    \n[50]\tvalidation_0-logloss:0.00680\tvalidation_1-logloss:0.16248                    \n[51]\tvalidation_0-logloss:0.00675\tvalidation_1-logloss:0.16146                    \n[52]\tvalidation_0-logloss:0.00670\tvalidation_1-logloss:0.16101                    \n[53]\tvalidation_0-logloss:0.00665\tvalidation_1-logloss:0.16173                    \n[54]\tvalidation_0-logloss:0.00660\tvalidation_1-logloss:0.16245                    \n[55]\tvalidation_0-logloss:0.00656\tvalidation_1-logloss:0.16312                    \n[56]\tvalidation_0-logloss:0.00651\tvalidation_1-logloss:0.16269                    \n[57]\tvalidation_0-logloss:0.00647\tvalidation_1-logloss:0.16237                    \n[58]\tvalidation_0-logloss:0.00642\tvalidation_1-logloss:0.16197                    \n[59]\tvalidation_0-logloss:0.00638\tvalidation_1-logloss:0.16167                    \n[60]\tvalidation_0-logloss:0.00634\tvalidation_1-logloss:0.16232                    \n[61]\tvalidation_0-logloss:0.00630\tvalidation_1-logloss:0.16208                    \n[62]\tvalidation_0-logloss:0.00626\tvalidation_1-logloss:0.16199                    \n[63]\tvalidation_0-logloss:0.00622\tvalidation_1-logloss:0.16244                    \n[64]\tvalidation_0-logloss:0.00619\tvalidation_1-logloss:0.16219                    \n[65]\tvalidation_0-logloss:0.00615\tvalidation_1-logloss:0.16264                    \n[66]\tvalidation_0-logloss:0.00611\tvalidation_1-logloss:0.16327                    \n[67]\tvalidation_0-logloss:0.00608\tvalidation_1-logloss:0.16302                    \n[68]\tvalidation_0-logloss:0.00604\tvalidation_1-logloss:0.16378                    \n[69]\tvalidation_0-logloss:0.00601\tvalidation_1-logloss:0.16441                    \n[70]\tvalidation_0-logloss:0.00598\tvalidation_1-logloss:0.16455                    \n[71]\tvalidation_0-logloss:0.00595\tvalidation_1-logloss:0.16516                    \n[72]\tvalidation_0-logloss:0.00591\tvalidation_1-logloss:0.16478                    \n[73]\tvalidation_0-logloss:0.00588\tvalidation_1-logloss:0.16533                    \n[74]\tvalidation_0-logloss:0.00585\tvalidation_1-logloss:0.16513                    \n[75]\tvalidation_0-logloss:0.00582\tvalidation_1-logloss:0.16400                    \n[76]\tvalidation_0-logloss:0.00580\tvalidation_1-logloss:0.16468                    \n[77]\tvalidation_0-logloss:0.00577\tvalidation_1-logloss:0.16529                    \n[78]\tvalidation_0-logloss:0.00574\tvalidation_1-logloss:0.16570                    \n[79]\tvalidation_0-logloss:0.00571\tvalidation_1-logloss:0.16626                    \n[80]\tvalidation_0-logloss:0.00568\tvalidation_1-logloss:0.16591                    \n[81]\tvalidation_0-logloss:0.00566\tvalidation_1-logloss:0.16555                    \n[82]\tvalidation_0-logloss:0.00563\tvalidation_1-logloss:0.16449                    \n[83]\tvalidation_0-logloss:0.00561\tvalidation_1-logloss:0.16502                    \n[84]\tvalidation_0-logloss:0.00559\tvalidation_1-logloss:0.16543                    \n[85]\tvalidation_0-logloss:0.00557\tvalidation_1-logloss:0.16608                    \n[86]\tvalidation_0-logloss:0.00554\tvalidation_1-logloss:0.16660                    \n[87]\tvalidation_0-logloss:0.00552\tvalidation_1-logloss:0.16641                    \n[88]\tvalidation_0-logloss:0.00550\tvalidation_1-logloss:0.16688                    \n[89]\tvalidation_0-logloss:0.00548\tvalidation_1-logloss:0.16657                    \n[90]\tvalidation_0-logloss:0.00546\tvalidation_1-logloss:0.16688                    \n[91]\tvalidation_0-logloss:0.00544\tvalidation_1-logloss:0.16739                    \n[92]\tvalidation_0-logloss:0.00542\tvalidation_1-logloss:0.16653                    \n[93]\tvalidation_0-logloss:0.00541\tvalidation_1-logloss:0.16674                    \n[94]\tvalidation_0-logloss:0.00539\tvalidation_1-logloss:0.16690                    \n[95]\tvalidation_0-logloss:0.00537\tvalidation_1-logloss:0.16719                    \n[96]\tvalidation_0-logloss:0.00535\tvalidation_1-logloss:0.16756                    \n[97]\tvalidation_0-logloss:0.00534\tvalidation_1-logloss:0.16810                    \n[98]\tvalidation_0-logloss:0.00532\tvalidation_1-logloss:0.16756                    \n[99]\tvalidation_0-logloss:0.00531\tvalidation_1-logloss:0.16752                    \nSCORE: 0.9210526315789473                                                         \n[0]\tvalidation_0-logloss:0.43595\tvalidation_1-logloss:0.46791                     \n[1]\tvalidation_0-logloss:0.31069\tvalidation_1-logloss:0.37256                     \n[2]\tvalidation_0-logloss:0.23262\tvalidation_1-logloss:0.30338                     \n[3]\tvalidation_0-logloss:0.17765\tvalidation_1-logloss:0.26996                     \n[4]\tvalidation_0-logloss:0.13823\tvalidation_1-logloss:0.24162                     \n[5]\tvalidation_0-logloss:0.10928\tvalidation_1-logloss:0.21845                     \n[6]\tvalidation_0-logloss:0.08707\tvalidation_1-logloss:0.20883                     \n[7]\tvalidation_0-logloss:0.07166\tvalidation_1-logloss:0.20241                     \n[8]\tvalidation_0-logloss:0.05958\tvalidation_1-logloss:0.19466                     \n[9]\tvalidation_0-logloss:0.05026\tvalidation_1-logloss:0.18970                     \n[10]\tvalidation_0-logloss:0.04389\tvalidation_1-logloss:0.18636                    \n[11]\tvalidation_0-logloss:0.03820\tvalidation_1-logloss:0.18209                    \n[12]\tvalidation_0-logloss:0.03378\tvalidation_1-logloss:0.17523                    \n[13]\tvalidation_0-logloss:0.03037\tvalidation_1-logloss:0.17843                    \n[14]\tvalidation_0-logloss:0.02739\tvalidation_1-logloss:0.17832                    \n[15]\tvalidation_0-logloss:0.02491\tvalidation_1-logloss:0.17728                    \n[16]\tvalidation_0-logloss:0.02303\tvalidation_1-logloss:0.17653                    \n[17]\tvalidation_0-logloss:0.02134\tvalidation_1-logloss:0.17684                    \n[18]\tvalidation_0-logloss:0.01970\tvalidation_1-logloss:0.17284                    \n[19]\tvalidation_0-logloss:0.01856\tvalidation_1-logloss:0.17294                    \n[20]\tvalidation_0-logloss:0.01757\tvalidation_1-logloss:0.16786                    \n[21]\tvalidation_0-logloss:0.01686\tvalidation_1-logloss:0.16781                    \n[22]\tvalidation_0-logloss:0.01614\tvalidation_1-logloss:0.16413                    \n[23]\tvalidation_0-logloss:0.01537\tvalidation_1-logloss:0.16644                    \n[24]\tvalidation_0-logloss:0.01466\tvalidation_1-logloss:0.16578                    \n[25]\tvalidation_0-logloss:0.01392\tvalidation_1-logloss:0.16597                    \n[26]\tvalidation_0-logloss:0.01340\tvalidation_1-logloss:0.16745                    \n[27]\tvalidation_0-logloss:0.01293\tvalidation_1-logloss:0.16652                    \n[28]\tvalidation_0-logloss:0.01252\tvalidation_1-logloss:0.16380                    \n[29]\tvalidation_0-logloss:0.01205\tvalidation_1-logloss:0.16349                    \n[30]\tvalidation_0-logloss:0.01166\tvalidation_1-logloss:0.16312                    \n[31]\tvalidation_0-logloss:0.01132\tvalidation_1-logloss:0.16583                    \n[32]\tvalidation_0-logloss:0.01106\tvalidation_1-logloss:0.16371                    \n[33]\tvalidation_0-logloss:0.01073\tvalidation_1-logloss:0.16230                    \n[34]\tvalidation_0-logloss:0.01046\tvalidation_1-logloss:0.16018                    \n[35]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[36]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[37]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[38]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[39]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[40]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[41]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[42]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[43]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[44]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[45]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[46]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[47]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[48]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[49]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[50]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[51]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[52]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[53]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[54]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[55]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[56]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[57]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[58]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[59]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[60]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[61]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[62]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[63]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[64]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[65]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[66]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[67]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[68]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[69]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[70]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[71]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[72]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[73]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[74]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[75]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[76]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[77]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[78]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[79]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[80]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[81]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[82]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[83]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[84]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[85]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[86]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[87]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[88]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[89]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[90]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[91]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[92]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[93]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[94]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[95]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[96]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[97]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[98]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \n[99]\tvalidation_0-logloss:0.01021\tvalidation_1-logloss:0.16016                    \nSCORE: 0.9210526315789473                                                         \n[0]\tvalidation_0-logloss:0.43397\tvalidation_1-logloss:0.45564                     \n[1]\tvalidation_0-logloss:0.30905\tvalidation_1-logloss:0.35535                     \n[2]\tvalidation_0-logloss:0.22786\tvalidation_1-logloss:0.29473                     \n[3]\tvalidation_0-logloss:0.17270\tvalidation_1-logloss:0.25362                     \n[4]\tvalidation_0-logloss:0.13443\tvalidation_1-logloss:0.22205                     \n                                                                                  \r","output_type":"stream"}],"execution_count":null}]}