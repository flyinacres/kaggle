{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# XGB Optimization\n\nExperiment with XGB parameter optimization to see if it can be used with my Abalone experiments.\n\nhttps://medium.com/@attud_bidirt/automatic-tuning-of-hyper-parameters-of-a-xgboost-classifier-c5588bceda4\n\nNote that the code in this example does not work in the order specified.  The code blocks are misarranged, and there is some missing work.  But still interesting!","metadata":{}},{"cell_type":"markdown","source":"## Get data right","metadata":{}},{"cell_type":"code","source":"from sklearn import datasets\n\n# without the as frame this is loaded as Bunch\ndata = datasets.load_breast_cancer(as_frame=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T20:47:28.409462Z","iopub.execute_input":"2025-01-23T20:47:28.409777Z","iopub.status.idle":"2025-01-23T20:47:30.771190Z","shell.execute_reply.started":"2025-01-23T20:47:28.409749Z","shell.execute_reply":"2025-01-23T20:47:30.770044Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"type(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:36:56.757911Z","iopub.execute_input":"2025-01-22T08:36:56.758658Z","iopub.status.idle":"2025-01-22T08:36:56.764652Z","shell.execute_reply.started":"2025-01-22T08:36:56.758607Z","shell.execute_reply":"2025-01-22T08:36:56.763577Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"sklearn.utils._bunch.Bunch"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"data.frame","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:37:29.933467Z","iopub.execute_input":"2025-01-22T08:37:29.933803Z","iopub.status.idle":"2025-01-22T08:37:29.968911Z","shell.execute_reply.started":"2025-01-22T08:37:29.933778Z","shell.execute_reply":"2025-01-22T08:37:29.967564Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n0          17.99         10.38          122.80     1001.0          0.11840   \n1          20.57         17.77          132.90     1326.0          0.08474   \n2          19.69         21.25          130.00     1203.0          0.10960   \n3          11.42         20.38           77.58      386.1          0.14250   \n4          20.29         14.34          135.10     1297.0          0.10030   \n..           ...           ...             ...        ...              ...   \n564        21.56         22.39          142.00     1479.0          0.11100   \n565        20.13         28.25          131.20     1261.0          0.09780   \n566        16.60         28.08          108.30      858.1          0.08455   \n567        20.60         29.33          140.10     1265.0          0.11780   \n568         7.76         24.54           47.92      181.0          0.05263   \n\n     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n0             0.27760         0.30010              0.14710         0.2419   \n1             0.07864         0.08690              0.07017         0.1812   \n2             0.15990         0.19740              0.12790         0.2069   \n3             0.28390         0.24140              0.10520         0.2597   \n4             0.13280         0.19800              0.10430         0.1809   \n..                ...             ...                  ...            ...   \n564           0.11590         0.24390              0.13890         0.1726   \n565           0.10340         0.14400              0.09791         0.1752   \n566           0.10230         0.09251              0.05302         0.1590   \n567           0.27700         0.35140              0.15200         0.2397   \n568           0.04362         0.00000              0.00000         0.1587   \n\n     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n0                   0.07871  ...          17.33           184.60      2019.0   \n1                   0.05667  ...          23.41           158.80      1956.0   \n2                   0.05999  ...          25.53           152.50      1709.0   \n3                   0.09744  ...          26.50            98.87       567.7   \n4                   0.05883  ...          16.67           152.20      1575.0   \n..                      ...  ...            ...              ...         ...   \n564                 0.05623  ...          26.40           166.10      2027.0   \n565                 0.05533  ...          38.25           155.00      1731.0   \n566                 0.05648  ...          34.12           126.70      1124.0   \n567                 0.07016  ...          39.42           184.60      1821.0   \n568                 0.05884  ...          30.37            59.16       268.6   \n\n     worst smoothness  worst compactness  worst concavity  \\\n0             0.16220            0.66560           0.7119   \n1             0.12380            0.18660           0.2416   \n2             0.14440            0.42450           0.4504   \n3             0.20980            0.86630           0.6869   \n4             0.13740            0.20500           0.4000   \n..                ...                ...              ...   \n564           0.14100            0.21130           0.4107   \n565           0.11660            0.19220           0.3215   \n566           0.11390            0.30940           0.3403   \n567           0.16500            0.86810           0.9387   \n568           0.08996            0.06444           0.0000   \n\n     worst concave points  worst symmetry  worst fractal dimension  target  \n0                  0.2654          0.4601                  0.11890       0  \n1                  0.1860          0.2750                  0.08902       0  \n2                  0.2430          0.3613                  0.08758       0  \n3                  0.2575          0.6638                  0.17300       0  \n4                  0.1625          0.2364                  0.07678       0  \n..                    ...             ...                      ...     ...  \n564                0.2216          0.2060                  0.07115       0  \n565                0.1628          0.2572                  0.06637       0  \n566                0.1418          0.2218                  0.07820       0  \n567                0.2650          0.4087                  0.12400       0  \n568                0.0000          0.2871                  0.07039       1  \n\n[569 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean radius</th>\n      <th>mean texture</th>\n      <th>mean perimeter</th>\n      <th>mean area</th>\n      <th>mean smoothness</th>\n      <th>mean compactness</th>\n      <th>mean concavity</th>\n      <th>mean concave points</th>\n      <th>mean symmetry</th>\n      <th>mean fractal dimension</th>\n      <th>...</th>\n      <th>worst texture</th>\n      <th>worst perimeter</th>\n      <th>worst area</th>\n      <th>worst smoothness</th>\n      <th>worst compactness</th>\n      <th>worst concavity</th>\n      <th>worst concave points</th>\n      <th>worst symmetry</th>\n      <th>worst fractal dimension</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.30010</td>\n      <td>0.14710</td>\n      <td>0.2419</td>\n      <td>0.07871</td>\n      <td>...</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.16220</td>\n      <td>0.66560</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.08690</td>\n      <td>0.07017</td>\n      <td>0.1812</td>\n      <td>0.05667</td>\n      <td>...</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.12380</td>\n      <td>0.18660</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.19740</td>\n      <td>0.12790</td>\n      <td>0.2069</td>\n      <td>0.05999</td>\n      <td>...</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.14440</td>\n      <td>0.42450</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.24140</td>\n      <td>0.10520</td>\n      <td>0.2597</td>\n      <td>0.09744</td>\n      <td>...</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.20980</td>\n      <td>0.86630</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.19800</td>\n      <td>0.10430</td>\n      <td>0.1809</td>\n      <td>0.05883</td>\n      <td>...</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.13740</td>\n      <td>0.20500</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>564</th>\n      <td>21.56</td>\n      <td>22.39</td>\n      <td>142.00</td>\n      <td>1479.0</td>\n      <td>0.11100</td>\n      <td>0.11590</td>\n      <td>0.24390</td>\n      <td>0.13890</td>\n      <td>0.1726</td>\n      <td>0.05623</td>\n      <td>...</td>\n      <td>26.40</td>\n      <td>166.10</td>\n      <td>2027.0</td>\n      <td>0.14100</td>\n      <td>0.21130</td>\n      <td>0.4107</td>\n      <td>0.2216</td>\n      <td>0.2060</td>\n      <td>0.07115</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>565</th>\n      <td>20.13</td>\n      <td>28.25</td>\n      <td>131.20</td>\n      <td>1261.0</td>\n      <td>0.09780</td>\n      <td>0.10340</td>\n      <td>0.14400</td>\n      <td>0.09791</td>\n      <td>0.1752</td>\n      <td>0.05533</td>\n      <td>...</td>\n      <td>38.25</td>\n      <td>155.00</td>\n      <td>1731.0</td>\n      <td>0.11660</td>\n      <td>0.19220</td>\n      <td>0.3215</td>\n      <td>0.1628</td>\n      <td>0.2572</td>\n      <td>0.06637</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>566</th>\n      <td>16.60</td>\n      <td>28.08</td>\n      <td>108.30</td>\n      <td>858.1</td>\n      <td>0.08455</td>\n      <td>0.10230</td>\n      <td>0.09251</td>\n      <td>0.05302</td>\n      <td>0.1590</td>\n      <td>0.05648</td>\n      <td>...</td>\n      <td>34.12</td>\n      <td>126.70</td>\n      <td>1124.0</td>\n      <td>0.11390</td>\n      <td>0.30940</td>\n      <td>0.3403</td>\n      <td>0.1418</td>\n      <td>0.2218</td>\n      <td>0.07820</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>567</th>\n      <td>20.60</td>\n      <td>29.33</td>\n      <td>140.10</td>\n      <td>1265.0</td>\n      <td>0.11780</td>\n      <td>0.27700</td>\n      <td>0.35140</td>\n      <td>0.15200</td>\n      <td>0.2397</td>\n      <td>0.07016</td>\n      <td>...</td>\n      <td>39.42</td>\n      <td>184.60</td>\n      <td>1821.0</td>\n      <td>0.16500</td>\n      <td>0.86810</td>\n      <td>0.9387</td>\n      <td>0.2650</td>\n      <td>0.4087</td>\n      <td>0.12400</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>568</th>\n      <td>7.76</td>\n      <td>24.54</td>\n      <td>47.92</td>\n      <td>181.0</td>\n      <td>0.05263</td>\n      <td>0.04362</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.1587</td>\n      <td>0.05884</td>\n      <td>...</td>\n      <td>30.37</td>\n      <td>59.16</td>\n      <td>268.6</td>\n      <td>0.08996</td>\n      <td>0.06444</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.2871</td>\n      <td>0.07039</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>569 rows × 31 columns</p>\n</div>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"data.frame.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:37:57.312217Z","iopub.execute_input":"2025-01-22T08:37:57.312511Z","iopub.status.idle":"2025-01-22T08:37:57.318572Z","shell.execute_reply.started":"2025-01-22T08:37:57.312492Z","shell.execute_reply":"2025-01-22T08:37:57.317633Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"Index(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n       'mean smoothness', 'mean compactness', 'mean concavity',\n       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n       'radius error', 'texture error', 'perimeter error', 'area error',\n       'smoothness error', 'compactness error', 'concavity error',\n       'concave points error', 'symmetry error', 'fractal dimension error',\n       'worst radius', 'worst texture', 'worst perimeter', 'worst area',\n       'worst smoothness', 'worst compactness', 'worst concavity',\n       'worst concave points', 'worst symmetry', 'worst fractal dimension',\n       'target'],\n      dtype='object')"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"data.frame.target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:39:27.917817Z","iopub.execute_input":"2025-01-22T08:39:27.918182Z","iopub.status.idle":"2025-01-22T08:39:27.925945Z","shell.execute_reply.started":"2025-01-22T08:39:27.918158Z","shell.execute_reply":"2025-01-22T08:39:27.924120Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"0      0\n1      0\n2      0\n3      0\n4      0\n      ..\n564    0\n565    0\n566    0\n567    0\n568    1\nName: target, Length: 569, dtype: int64"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"# Need to separate data into X and Y\nX = data.frame.drop('target', axis = 1)\ny = data.frame.target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T20:47:45.027306Z","iopub.execute_input":"2025-01-23T20:47:45.027753Z","iopub.status.idle":"2025-01-23T20:47:45.033979Z","shell.execute_reply.started":"2025-01-23T20:47:45.027717Z","shell.execute_reply":"2025-01-23T20:47:45.032680Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:41:16.116811Z","iopub.execute_input":"2025-01-22T08:41:16.117187Z","iopub.status.idle":"2025-01-22T08:41:16.151503Z","shell.execute_reply.started":"2025-01-22T08:41:16.117156Z","shell.execute_reply":"2025-01-22T08:41:16.147890Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n0          17.99         10.38          122.80     1001.0          0.11840   \n1          20.57         17.77          132.90     1326.0          0.08474   \n2          19.69         21.25          130.00     1203.0          0.10960   \n3          11.42         20.38           77.58      386.1          0.14250   \n4          20.29         14.34          135.10     1297.0          0.10030   \n..           ...           ...             ...        ...              ...   \n564        21.56         22.39          142.00     1479.0          0.11100   \n565        20.13         28.25          131.20     1261.0          0.09780   \n566        16.60         28.08          108.30      858.1          0.08455   \n567        20.60         29.33          140.10     1265.0          0.11780   \n568         7.76         24.54           47.92      181.0          0.05263   \n\n     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n0             0.27760         0.30010              0.14710         0.2419   \n1             0.07864         0.08690              0.07017         0.1812   \n2             0.15990         0.19740              0.12790         0.2069   \n3             0.28390         0.24140              0.10520         0.2597   \n4             0.13280         0.19800              0.10430         0.1809   \n..                ...             ...                  ...            ...   \n564           0.11590         0.24390              0.13890         0.1726   \n565           0.10340         0.14400              0.09791         0.1752   \n566           0.10230         0.09251              0.05302         0.1590   \n567           0.27700         0.35140              0.15200         0.2397   \n568           0.04362         0.00000              0.00000         0.1587   \n\n     mean fractal dimension  ...  worst radius  worst texture  \\\n0                   0.07871  ...        25.380          17.33   \n1                   0.05667  ...        24.990          23.41   \n2                   0.05999  ...        23.570          25.53   \n3                   0.09744  ...        14.910          26.50   \n4                   0.05883  ...        22.540          16.67   \n..                      ...  ...           ...            ...   \n564                 0.05623  ...        25.450          26.40   \n565                 0.05533  ...        23.690          38.25   \n566                 0.05648  ...        18.980          34.12   \n567                 0.07016  ...        25.740          39.42   \n568                 0.05884  ...         9.456          30.37   \n\n     worst perimeter  worst area  worst smoothness  worst compactness  \\\n0             184.60      2019.0           0.16220            0.66560   \n1             158.80      1956.0           0.12380            0.18660   \n2             152.50      1709.0           0.14440            0.42450   \n3              98.87       567.7           0.20980            0.86630   \n4             152.20      1575.0           0.13740            0.20500   \n..               ...         ...               ...                ...   \n564           166.10      2027.0           0.14100            0.21130   \n565           155.00      1731.0           0.11660            0.19220   \n566           126.70      1124.0           0.11390            0.30940   \n567           184.60      1821.0           0.16500            0.86810   \n568            59.16       268.6           0.08996            0.06444   \n\n     worst concavity  worst concave points  worst symmetry  \\\n0             0.7119                0.2654          0.4601   \n1             0.2416                0.1860          0.2750   \n2             0.4504                0.2430          0.3613   \n3             0.6869                0.2575          0.6638   \n4             0.4000                0.1625          0.2364   \n..               ...                   ...             ...   \n564           0.4107                0.2216          0.2060   \n565           0.3215                0.1628          0.2572   \n566           0.3403                0.1418          0.2218   \n567           0.9387                0.2650          0.4087   \n568           0.0000                0.0000          0.2871   \n\n     worst fractal dimension  \n0                    0.11890  \n1                    0.08902  \n2                    0.08758  \n3                    0.17300  \n4                    0.07678  \n..                       ...  \n564                  0.07115  \n565                  0.06637  \n566                  0.07820  \n567                  0.12400  \n568                  0.07039  \n\n[569 rows x 30 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean radius</th>\n      <th>mean texture</th>\n      <th>mean perimeter</th>\n      <th>mean area</th>\n      <th>mean smoothness</th>\n      <th>mean compactness</th>\n      <th>mean concavity</th>\n      <th>mean concave points</th>\n      <th>mean symmetry</th>\n      <th>mean fractal dimension</th>\n      <th>...</th>\n      <th>worst radius</th>\n      <th>worst texture</th>\n      <th>worst perimeter</th>\n      <th>worst area</th>\n      <th>worst smoothness</th>\n      <th>worst compactness</th>\n      <th>worst concavity</th>\n      <th>worst concave points</th>\n      <th>worst symmetry</th>\n      <th>worst fractal dimension</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.30010</td>\n      <td>0.14710</td>\n      <td>0.2419</td>\n      <td>0.07871</td>\n      <td>...</td>\n      <td>25.380</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.16220</td>\n      <td>0.66560</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.08690</td>\n      <td>0.07017</td>\n      <td>0.1812</td>\n      <td>0.05667</td>\n      <td>...</td>\n      <td>24.990</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.12380</td>\n      <td>0.18660</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.19740</td>\n      <td>0.12790</td>\n      <td>0.2069</td>\n      <td>0.05999</td>\n      <td>...</td>\n      <td>23.570</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.14440</td>\n      <td>0.42450</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.24140</td>\n      <td>0.10520</td>\n      <td>0.2597</td>\n      <td>0.09744</td>\n      <td>...</td>\n      <td>14.910</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.20980</td>\n      <td>0.86630</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.19800</td>\n      <td>0.10430</td>\n      <td>0.1809</td>\n      <td>0.05883</td>\n      <td>...</td>\n      <td>22.540</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.13740</td>\n      <td>0.20500</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>564</th>\n      <td>21.56</td>\n      <td>22.39</td>\n      <td>142.00</td>\n      <td>1479.0</td>\n      <td>0.11100</td>\n      <td>0.11590</td>\n      <td>0.24390</td>\n      <td>0.13890</td>\n      <td>0.1726</td>\n      <td>0.05623</td>\n      <td>...</td>\n      <td>25.450</td>\n      <td>26.40</td>\n      <td>166.10</td>\n      <td>2027.0</td>\n      <td>0.14100</td>\n      <td>0.21130</td>\n      <td>0.4107</td>\n      <td>0.2216</td>\n      <td>0.2060</td>\n      <td>0.07115</td>\n    </tr>\n    <tr>\n      <th>565</th>\n      <td>20.13</td>\n      <td>28.25</td>\n      <td>131.20</td>\n      <td>1261.0</td>\n      <td>0.09780</td>\n      <td>0.10340</td>\n      <td>0.14400</td>\n      <td>0.09791</td>\n      <td>0.1752</td>\n      <td>0.05533</td>\n      <td>...</td>\n      <td>23.690</td>\n      <td>38.25</td>\n      <td>155.00</td>\n      <td>1731.0</td>\n      <td>0.11660</td>\n      <td>0.19220</td>\n      <td>0.3215</td>\n      <td>0.1628</td>\n      <td>0.2572</td>\n      <td>0.06637</td>\n    </tr>\n    <tr>\n      <th>566</th>\n      <td>16.60</td>\n      <td>28.08</td>\n      <td>108.30</td>\n      <td>858.1</td>\n      <td>0.08455</td>\n      <td>0.10230</td>\n      <td>0.09251</td>\n      <td>0.05302</td>\n      <td>0.1590</td>\n      <td>0.05648</td>\n      <td>...</td>\n      <td>18.980</td>\n      <td>34.12</td>\n      <td>126.70</td>\n      <td>1124.0</td>\n      <td>0.11390</td>\n      <td>0.30940</td>\n      <td>0.3403</td>\n      <td>0.1418</td>\n      <td>0.2218</td>\n      <td>0.07820</td>\n    </tr>\n    <tr>\n      <th>567</th>\n      <td>20.60</td>\n      <td>29.33</td>\n      <td>140.10</td>\n      <td>1265.0</td>\n      <td>0.11780</td>\n      <td>0.27700</td>\n      <td>0.35140</td>\n      <td>0.15200</td>\n      <td>0.2397</td>\n      <td>0.07016</td>\n      <td>...</td>\n      <td>25.740</td>\n      <td>39.42</td>\n      <td>184.60</td>\n      <td>1821.0</td>\n      <td>0.16500</td>\n      <td>0.86810</td>\n      <td>0.9387</td>\n      <td>0.2650</td>\n      <td>0.4087</td>\n      <td>0.12400</td>\n    </tr>\n    <tr>\n      <th>568</th>\n      <td>7.76</td>\n      <td>24.54</td>\n      <td>47.92</td>\n      <td>181.0</td>\n      <td>0.05263</td>\n      <td>0.04362</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.1587</td>\n      <td>0.05884</td>\n      <td>...</td>\n      <td>9.456</td>\n      <td>30.37</td>\n      <td>59.16</td>\n      <td>268.6</td>\n      <td>0.08996</td>\n      <td>0.06444</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.2871</td>\n      <td>0.07039</td>\n    </tr>\n  </tbody>\n</table>\n<p>569 rows × 30 columns</p>\n</div>"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:41:27.995161Z","iopub.execute_input":"2025-01-22T08:41:27.995451Z","iopub.status.idle":"2025-01-22T08:41:28.003148Z","shell.execute_reply.started":"2025-01-22T08:41:27.995433Z","shell.execute_reply":"2025-01-22T08:41:28.001707Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"0      0\n1      0\n2      0\n3      0\n4      0\n      ..\n564    0\n565    0\n566    0\n567    0\n568    1\nName: target, Length: 569, dtype: int64"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"from pandas.core.common import random_state\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nss = StratifiedShuffleSplit(2,test_size=0.2, random_state=44)\nfor tr_idx,ts_idx in ss.split(X,y):\n  X_train, y_train = X.loc[tr_idx], y.loc[tr_idx]\n  X_test, y_test = X.loc[ts_idx], y.loc[ts_idx]\n\nprint(f\"\\nShape of X_train is {X_train.shape}\")\nprint(f\"\\nShape of X_test is {X_test.shape}\")\nprint(f\"\\nLength of y_train is {y_train.shape}\")\nprint(f\"\\nLength of y_test is {y_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T20:47:53.548386Z","iopub.execute_input":"2025-01-23T20:47:53.548772Z","iopub.status.idle":"2025-01-23T20:47:53.692268Z","shell.execute_reply.started":"2025-01-23T20:47:53.548743Z","shell.execute_reply":"2025-01-23T20:47:53.691039Z"}},"outputs":[{"name":"stdout","text":"\nShape of X_train is (455, 30)\n\nShape of X_test is (114, 30)\n\nLength of y_train is (455,)\n\nLength of y_test is (114,)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Preliminary Tests","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb \nfrom sklearn.model_selection import cross_val_score \nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe, space_eval \nfrom hyperopt.early_stop import no_progress_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:42:32.140627Z","iopub.execute_input":"2025-01-22T08:42:32.141014Z","iopub.status.idle":"2025-01-22T08:42:33.763498Z","shell.execute_reply.started":"2025-01-22T08:42:32.140982Z","shell.execute_reply":"2025-01-22T08:42:33.762368Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"dtrain_clf = xgb.DMatrix(X_train, y_train, enable_categorical = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:43:26.865849Z","iopub.execute_input":"2025-01-22T08:43:26.866229Z","iopub.status.idle":"2025-01-22T08:43:26.904912Z","shell.execute_reply.started":"2025-01-22T08:43:26.866210Z","shell.execute_reply":"2025-01-22T08:43:26.903359Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"from sklearn.metrics import f1_score, recall_score, confusion_matrix,roc_auc_score\n\nparams_1 = {\"objective\": \"binary:logistic\"}\n\nn = 1000\n\nresults = xgb.cv(params_1,\n                 dtrain_clf,\n                 num_boost_round = n,\n                 nfold=5,\n                 metrics = [\"logloss\",\"auc\",\"error\"],\n                 early_stopping_rounds=20\n                 )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:43:29.674751Z","iopub.execute_input":"2025-01-22T08:43:29.675077Z","iopub.status.idle":"2025-01-22T08:43:30.235628Z","shell.execute_reply.started":"2025-01-22T08:43:29.675058Z","shell.execute_reply":"2025-01-22T08:43:30.234949Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"clf_1 = xgb.XGBClassifier(**params_1)\n\nclf_1.fit(X_train,y_train)\n\npred_1 = clf_1.predict(X_test)\n\nprint(f\"f1 score : {f1_score(y_test, pred_1)}\\n\")\nprint(f\"confusion Matrix:\\n{confusion_matrix(y_test, pred_1)}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:44:37.400611Z","iopub.execute_input":"2025-01-22T08:44:37.400910Z","iopub.status.idle":"2025-01-22T08:44:37.492274Z","shell.execute_reply.started":"2025-01-22T08:44:37.400881Z","shell.execute_reply":"2025-01-22T08:44:37.491225Z"}},"outputs":[{"name":"stdout","text":"f1 score : 0.9403973509933775\n\nconfusion Matrix:\n[[34  8]\n [ 1 71]]\n\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"## Now Hyperopt","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb \nfrom sklearn.model_selection import cross_val_score \nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe, space_eval \nfrom hyperopt.early_stop import no_progress_loss\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:47:46.097651Z","iopub.execute_input":"2025-01-22T08:47:46.097978Z","iopub.status.idle":"2025-01-22T08:47:46.102675Z","shell.execute_reply.started":"2025-01-22T08:47:46.097953Z","shell.execute_reply":"2025-01-22T08:47:46.101335Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"search_space = {\n    'max_depth': hp.choice(\"max_depth\", np.arange(1,20,1,dtype=int)),\n    'eta'      : hp.uniform(\"eta\", 0, 1),\n    'gamma'    : hp.uniform(\"gamma\", 0, 10e1),\n    'reg_alpha': hp.uniform(\"reg_alpha\", 10e-7, 10),\n    'reg_lambda' : hp.uniform(\"reg_lambda\", 0,1),\n    'colsample_bytree': hp.uniform(\"colsample_bytree\", 0.5,1),\n    'colsample_bynode': hp.uniform(\"colsample_bynode\", 0.5,1), \n    'colsample_bylevel': hp.uniform(\"colsample_bylevel\", 0.5,1),\n    'n_estimators': hp.choice(\"n_estimators\", np.arange(100,1000,10,dtype='int')),\n    'min_child_weight' : hp.choice(\"min_child_weight\", np.arange(1,10,1,dtype='int')),\n    'max_delta_step' : hp.choice(\"max_delta_step\", np.arange(1,10,1,dtype='int')),\n    'subsample' : hp.uniform(\"subsample\",0.5,1),\n    'objective' : 'binary:logistic',\n    'eval_metric' : 'aucpr',\n    'seed' : 44\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:47:49.081511Z","iopub.execute_input":"2025-01-22T08:47:49.081857Z","iopub.status.idle":"2025-01-22T08:47:49.088999Z","shell.execute_reply.started":"2025-01-22T08:47:49.081828Z","shell.execute_reply":"2025-01-22T08:47:49.087345Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"def xgb_objective(space):\n  results = xgb.cv(space, \n                   dtrain=dtrain_clf, #DMatrix (xgboost specific)\n                   num_boost_round=500, \n                   nfold=5, \n                   stratified=True,  \n                   early_stopping_rounds=20,\n                   metrics = ['logloss','auc','aucpr','error'])\n  \n  best_score = results['test-auc-mean'].max()\n  return {'loss':-best_score, 'status': STATUS_OK}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:48:06.987544Z","iopub.execute_input":"2025-01-22T08:48:06.987886Z","iopub.status.idle":"2025-01-22T08:48:06.994694Z","shell.execute_reply.started":"2025-01-22T08:48:06.987867Z","shell.execute_reply":"2025-01-22T08:48:06.991559Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"trials = Trials()\n\nbest_hyperparams = fmin(fn=xgb_objective, space=search_space,algo=tpe.suggest,max_evals=500,trials=trials, return_argmin=False, early_stop_fn=no_progress_loss(10))\n\nbest_params = best_hyperparams.copy()\n\n# `eval_metric` is a key that is not a hyperparameter of the classifier\nif 'eval_metric' in best_params:\n  best_params = {key:best_params[key] for key in best_params if key!='eval_metric'}\n\nbest_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:48:23.242795Z","iopub.execute_input":"2025-01-22T08:48:23.243160Z","iopub.status.idle":"2025-01-22T08:48:27.957421Z","shell.execute_reply.started":"2025-01-22T08:48:23.243133Z","shell.execute_reply":"2025-01-22T08:48:27.956311Z"}},"outputs":[{"name":"stdout","text":"  0%|          | 1/500 [00:00<00:54,  9.16trial/s, best loss: -0.8919504643962849]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:23] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:23] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  1%|          | 3/500 [00:00<01:02,  7.93trial/s, best loss: -0.9722910216718266]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:23] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:23] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  1%|          | 5/500 [00:00<01:08,  7.26trial/s, best loss: -0.972703818369453] ","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:23] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:23] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  1%|▏         | 7/500 [00:00<01:04,  7.69trial/s, best loss: -0.972703818369453]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:24] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:24] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  2%|▏         | 9/500 [00:01<01:07,  7.32trial/s, best loss: -0.9824561403508772]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:24] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:24] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  2%|▏         | 11/500 [00:01<01:27,  5.56trial/s, best loss: -0.986532507739938] ","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:24] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:24] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  2%|▏         | 12/500 [00:01<01:18,  6.22trial/s, best loss: -0.986532507739938]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:25] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  3%|▎         | 13/500 [00:01<01:28,  5.53trial/s, best loss: -0.986532507739938]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:25] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  3%|▎         | 15/500 [00:02<01:24,  5.75trial/s, best loss: -0.9919504643962849]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:25] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:25] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  3%|▎         | 17/500 [00:02<01:11,  6.72trial/s, best loss: -0.9919504643962849]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:25] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:26] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  4%|▍         | 19/500 [00:03<02:49,  2.85trial/s, best loss: -0.9919504643962849]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:27] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:27] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  4%|▍         | 21/500 [00:04<02:02,  3.91trial/s, best loss: -0.9919504643962849]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:27] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:27] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  5%|▍         | 23/500 [00:04<01:39,  4.81trial/s, best loss: -0.9919504643962849]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:27] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:48:27] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  5%|▍         | 24/500 [00:04<01:32,  5.13trial/s, best loss: -0.9919504643962849]\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"{'colsample_bylevel': 0.7370830882900052,\n 'colsample_bynode': 0.9456419018900569,\n 'colsample_bytree': 0.6886367856734236,\n 'eta': 0.5586879476999288,\n 'gamma': 1.9552221486075605,\n 'max_delta_step': 4,\n 'max_depth': 9,\n 'min_child_weight': 5,\n 'n_estimators': 630,\n 'objective': 'binary:logistic',\n 'reg_alpha': 0.939894791803691,\n 'reg_lambda': 0.5277342478394749,\n 'seed': 44,\n 'subsample': 0.5568365639214825}"},"metadata":{}}],"execution_count":37},{"cell_type":"markdown","source":"## Test with optimized params\n\nResults show that the 'best' params are meh. It may well have overfit","metadata":{}},{"cell_type":"code","source":"clf_2 = xgb.XGBClassifier(**best_params)\nclf_2.fit(X_train, y_train)\n\npred_2 = clf_2.predict(X_test)\n\nprint(f\"f1 score : {f1_score(y_test, pred_2)}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:49:23.123259Z","iopub.execute_input":"2025-01-22T08:49:23.123596Z","iopub.status.idle":"2025-01-22T08:49:23.316392Z","shell.execute_reply.started":"2025-01-22T08:49:23.123573Z","shell.execute_reply":"2025-01-22T08:49:23.314875Z"}},"outputs":[{"name":"stdout","text":"f1 score : 0.9459459459459458\n\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"search_space_3 = {\n    \n    'eta'      : hp.uniform(\"eta\", 0, 1),\n    'gamma'    : 0,#hp.uniform(\"gamma\", 0, 1),\n    'reg_lambda' : hp.uniform(\"reg_lambda\", 0,1),\n    'n_estimators': hp.choice(\"n_estimators\", np.arange(100,1000,10,dtype='int')),\n    'objective' : 'binary:logistic',\n    'eval_metric' : 'auc',\n    'seed' : 44\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:52:39.320038Z","iopub.execute_input":"2025-01-22T08:52:39.320421Z","iopub.status.idle":"2025-01-22T08:52:39.327110Z","shell.execute_reply.started":"2025-01-22T08:52:39.320400Z","shell.execute_reply":"2025-01-22T08:52:39.325741Z"}},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":"Hand tuned search space...","metadata":{}},{"cell_type":"code","source":"trials = Trials()\n\nbest_hyperparams = fmin(fn=xgb_objective, space=search_space_3,algo=tpe.suggest,max_evals=500,trials=trials, return_argmin=False, early_stop_fn=no_progress_loss(10))\n\nbest_params = best_hyperparams.copy()\n\n# `eval_metric` is a key that is not a hyperparameter of the classifier\nif 'eval_metric' in best_params:\n  best_params = {key:best_params[key] for key in best_params if key!='eval_metric'}\n\nbest_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:53:02.962941Z","iopub.execute_input":"2025-01-22T08:53:02.963379Z","iopub.status.idle":"2025-01-22T08:53:09.813165Z","shell.execute_reply.started":"2025-01-22T08:53:02.963345Z","shell.execute_reply":"2025-01-22T08:53:09.811836Z"}},"outputs":[{"name":"stdout","text":"  0%|          | 0/500 [00:00<?, ?trial/s, best loss=?]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:02] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:03] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  0%|          | 1/500 [00:00<01:57,  4.24trial/s, best loss: -0.9851909184726523]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:03] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  1%|          | 3/500 [00:01<02:50,  2.91trial/s, best loss: -0.9876160990712075]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:03] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:04] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  1%|          | 5/500 [00:01<02:04,  3.98trial/s, best loss: -0.9885964912280703]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:04] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:04] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  1%|▏         | 7/500 [00:01<02:00,  4.10trial/s, best loss: -0.9894736842105264]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:04] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:04] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  2%|▏         | 8/500 [00:02<02:06,  3.88trial/s, best loss: -0.9896284829721363]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:05] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  2%|▏         | 9/500 [00:02<02:03,  3.97trial/s, best loss: -0.9896284829721363]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:05] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  2%|▏         | 11/500 [00:02<01:49,  4.48trial/s, best loss: -0.9902476780185758]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:05] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:05] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  2%|▏         | 12/500 [00:03<02:12,  3.68trial/s, best loss: -0.9902476780185758]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:06] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  3%|▎         | 13/500 [00:03<02:08,  3.78trial/s, best loss: -0.9902476780185758]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:06] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  3%|▎         | 14/500 [00:03<02:06,  3.83trial/s, best loss: -0.9902476780185758]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:06] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  3%|▎         | 16/500 [00:04<02:11,  3.69trial/s, best loss: -0.9902476780185758]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:07] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:07] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  3%|▎         | 17/500 [00:04<02:20,  3.44trial/s, best loss: -0.9902476780185758]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:07] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  4%|▎         | 18/500 [00:04<02:12,  3.63trial/s, best loss: -0.9902476780185758]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:07] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  4%|▍         | 19/500 [00:06<05:09,  1.56trial/s, best loss: -0.9902476780185758]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [08:53:09] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n","output_type":"stream"},{"name":"stdout","text":"  4%|▍         | 20/500 [00:06<02:43,  2.93trial/s, best loss: -0.9902476780185758]\n","output_type":"stream"},{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"{'eta': 0.6783655045838408,\n 'gamma': 0,\n 'n_estimators': 890,\n 'objective': 'binary:logistic',\n 'reg_lambda': 0.3896554989724714,\n 'seed': 44}"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"clf_2 = xgb.XGBClassifier(**best_params)\nclf_2.fit(X_train, y_train)\n\npred_2 = clf_2.predict(X_test)\n\nprint(f\"f1 score : {f1_score(y_test, pred_2)}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T08:53:33.915319Z","iopub.execute_input":"2025-01-22T08:53:33.915651Z","iopub.status.idle":"2025-01-22T08:53:34.862227Z","shell.execute_reply.started":"2025-01-22T08:53:33.915624Z","shell.execute_reply":"2025-01-22T08:53:34.861445Z"}},"outputs":[{"name":"stdout","text":"f1 score : 0.953020134228188\n\n","output_type":"stream"}],"execution_count":46},{"cell_type":"markdown","source":"## TODO\n\nThis article is interesting buy almost disproves itselt, as the search for better params didn't lead to much.  I should play with hyperopt a bit more to see if I can figure out how to get it to add value.","metadata":{}},{"cell_type":"markdown","source":"## Take 2\n\nhttps://medium.com/@justin.wesley.johns/precision-ml-engineering-with-xgboost-hyperopt-attaining-98-11-accuracy-on-mnist-d737b7ef1081\n\n","metadata":{}},{"cell_type":"code","source":"pip install xgboost\npip install scikit-learn\npip install hyperopt\npip install tensorflow","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import xgboost as xgb\nfrom hyperopt import hp, fmin, tpe, STATUS_OK, Trials\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.datasets import mnist\nimport numpy as np","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the hyperparameter space\nspace = {\n    'max_depth': hp.quniform('max_depth', 3, 10, 1),\n    'gamma': hp.uniform('gamma', 0, 0.5),\n    'reg_alpha': hp.uniform('reg_alpha', 0, 0.1),\n    'reg_lambda': hp.uniform('reg_lambda', 0.5, 2),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n    'subsample': hp.uniform('subsample', 0.5, 1),\n    'n_estimators': hp.quniform('n_estimators', 100, 600, 50),\n    'learning_rate': hp.uniform('learning_rate', 0.05, 0.2),\n    'tree_method': 'gpu_hist'  # Use GPU\n}\n\n# Define the objective function\ndef objective(space):\n    clf = xgb.XGBClassifier(\n        n_estimators=int(space['n_estimators']),\n        max_depth=int(space['max_depth']),\n        gamma=space['gamma'],\n        reg_alpha=space['reg_alpha'],\n        reg_lambda=space['reg_lambda'],\n        colsample_bytree=space['colsample_bytree'],\n        subsample=space['subsample'],\n        learning_rate=space['learning_rate'],\n        tree_method=space['tree_method'],\n        use_label_encoder=False,  # Avoid the deprecation warning\n        eval_metric='mlogloss'\n    )\n    evaluation = [(x_train, y_train_encoded), (x_test, y_test_encoded)]\n        \n        clf.fit(x_train, y_train_encoded,\n                eval_set=evaluation, eval_metric=\"mlogloss\",\n                early_stopping_rounds=10, verbose=True)\n        \n        pred = clf.predict(x_test)\n        accuracy = accuracy_score(y_test_encoded, pred)\n        print(f\"SCORE: {accuracy}\")\n        return {'loss': -accuracy, 'status': STATUS_OK}\n\n# Run the optimization\ntrials = Trials()\nbest_hyperparams = fmin(fn=objective,\n                        space=space,\n                        algo=tpe.suggest,\n                        max_evals=200,\n                        trials=trials)\nprint(\"The best hyperparameters are: \", \"\\n\")\nprint(best_hyperparams)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}