{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10251383,"sourceType":"datasetVersion","datasetId":6340840}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I have another notebook where I examine data and explore various options.  \nHowever, that takes a while to run and is full of extraneous information.\nHere I distill everything down to a working example that I would like to submit...","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T23:36:12.410467Z","iopub.execute_input":"2024-12-26T23:36:12.410944Z","iopub.status.idle":"2024-12-26T23:36:12.417525Z","shell.execute_reply.started":"2024-12-26T23:36:12.410900Z","shell.execute_reply":"2024-12-26T23:36:12.415704Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"file_path = '/kaggle/input/abalone/train.csv'\ntrain_df = pd.read_csv(file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T23:36:14.471715Z","iopub.execute_input":"2024-12-26T23:36:14.472129Z","iopub.status.idle":"2024-12-26T23:36:14.592025Z","shell.execute_reply.started":"2024-12-26T23:36:14.472095Z","shell.execute_reply":"2024-12-26T23:36:14.590803Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# One-hot encode 'Sex'\ntrain_df = pd.get_dummies(train_df, columns=['Sex'], drop_first=True)\n\n# View the updated dataset\nprint(train_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T23:36:17.285937Z","iopub.execute_input":"2024-12-26T23:36:17.286331Z","iopub.status.idle":"2024-12-26T23:36:17.315355Z","shell.execute_reply.started":"2024-12-26T23:36:17.286294Z","shell.execute_reply":"2024-12-26T23:36:17.313740Z"}},"outputs":[{"name":"stdout","text":"   id  Length  Diameter  Height  Whole weight  Whole weight.1  Whole weight.2  \\\n0   0   0.550     0.430   0.150        0.7715          0.3285          0.1465   \n1   1   0.630     0.490   0.145        1.1300          0.4580          0.2765   \n2   2   0.160     0.110   0.025        0.0210          0.0055          0.0030   \n3   3   0.595     0.475   0.150        0.9145          0.3755          0.2055   \n4   4   0.555     0.425   0.130        0.7820          0.3695          0.1600   \n\n   Shell weight  Rings  Sex_I  Sex_M  \n0        0.2400     11  False  False  \n1        0.3200     11  False  False  \n2        0.0050      6   True  False  \n3        0.2500     10  False   True  \n4        0.1975      9   True  False  \n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# Retain only selected features\nselected_features = ['Shell weight', 'Height', 'Diameter', 'Whole weight', 'Rings', 'Sex_I', 'Sex_M']\ntrain_df_selected = train_df[selected_features]\n\n# Confirm the updated dataset\nprint(train_df_selected.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T23:36:20.016014Z","iopub.execute_input":"2024-12-26T23:36:20.016504Z","iopub.status.idle":"2024-12-26T23:36:20.029133Z","shell.execute_reply.started":"2024-12-26T23:36:20.016459Z","shell.execute_reply":"2024-12-26T23:36:20.027818Z"}},"outputs":[{"name":"stdout","text":"   Shell weight  Height  Diameter  Whole weight  Rings  Sex_I  Sex_M\n0        0.2400   0.150     0.430        0.7715     11  False  False\n1        0.3200   0.145     0.490        1.1300     11  False  False\n2        0.0050   0.025     0.110        0.0210      6   True  False\n3        0.2500   0.150     0.475        0.9145     10  False   True\n4        0.1975   0.130     0.425        0.7820      9   True  False\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Define features (X) and target (y)\nX = train_df_selected.drop(columns=['Rings'])  # Features\ny = train_df_selected['Rings']                # Target variable\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Confirm the split\nprint(\"Training set size:\", X_train.shape)\nprint(\"Testing set size:\", X_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T23:36:22.172096Z","iopub.execute_input":"2024-12-26T23:36:22.172512Z","iopub.status.idle":"2024-12-26T23:36:22.190246Z","shell.execute_reply.started":"2024-12-26T23:36:22.172480Z","shell.execute_reply":"2024-12-26T23:36:22.188817Z"}},"outputs":[{"name":"stdout","text":"Training set size: (72492, 6)\nTesting set size: (18123, 6)\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"Now prep the final test data in the same way\n\nNote that as this solution uses forests (and not linear regression) there is no value in using StandardScaler to scale data...","metadata":{}},{"cell_type":"code","source":"# Read the test data\nsubmission = pd.read_csv('/kaggle/input/abalone/test.csv')\n\n# One-hot encode 'Sex'\nsubmission_df = pd.get_dummies(submission, columns=['Sex'], drop_first=True)\n\nsubmission_df.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-26T23:36:25.788124Z","iopub.execute_input":"2024-12-26T23:36:25.788512Z","iopub.status.idle":"2024-12-26T23:36:25.891043Z","shell.execute_reply.started":"2024-12-26T23:36:25.788483Z","shell.execute_reply":"2024-12-26T23:36:25.889265Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"      id  Length  Diameter  Height  Whole weight  Whole weight.1  \\\n0  90615   0.645     0.475   0.155        1.2380          0.6185   \n1  90616   0.580     0.460   0.160        0.9830          0.4785   \n2  90617   0.560     0.420   0.140        0.8395          0.3525   \n3  90618   0.570     0.490   0.145        0.8740          0.3525   \n4  90619   0.415     0.325   0.110        0.3580          0.1575   \n\n   Whole weight.2  Shell weight  Sex_I  Sex_M  \n0          0.3125        0.3005  False   True  \n1          0.2195        0.2750  False   True  \n2          0.1845        0.2405  False   True  \n3          0.1865        0.2350  False   True  \n4          0.0670        0.1050   True  False  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Length</th>\n      <th>Diameter</th>\n      <th>Height</th>\n      <th>Whole weight</th>\n      <th>Whole weight.1</th>\n      <th>Whole weight.2</th>\n      <th>Shell weight</th>\n      <th>Sex_I</th>\n      <th>Sex_M</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>90615</td>\n      <td>0.645</td>\n      <td>0.475</td>\n      <td>0.155</td>\n      <td>1.2380</td>\n      <td>0.6185</td>\n      <td>0.3125</td>\n      <td>0.3005</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>90616</td>\n      <td>0.580</td>\n      <td>0.460</td>\n      <td>0.160</td>\n      <td>0.9830</td>\n      <td>0.4785</td>\n      <td>0.2195</td>\n      <td>0.2750</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>90617</td>\n      <td>0.560</td>\n      <td>0.420</td>\n      <td>0.140</td>\n      <td>0.8395</td>\n      <td>0.3525</td>\n      <td>0.1845</td>\n      <td>0.2405</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>90618</td>\n      <td>0.570</td>\n      <td>0.490</td>\n      <td>0.145</td>\n      <td>0.8740</td>\n      <td>0.3525</td>\n      <td>0.1865</td>\n      <td>0.2350</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>90619</td>\n      <td>0.415</td>\n      <td>0.325</td>\n      <td>0.110</td>\n      <td>0.3580</td>\n      <td>0.1575</td>\n      <td>0.0670</td>\n      <td>0.1050</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"test_selected_features = ['Shell weight', 'Height', 'Diameter', 'Whole weight', 'Sex_I', 'Sex_M']\n\nsubmission_selected = submission_df[test_selected_features]\n\n#submission_scaled = submission_selected.copy()\n#submission_scaled[numerical_features] = scaler.transform(submission_df[numerical_features])\n\nsubmission_selected.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T23:36:28.213760Z","iopub.execute_input":"2024-12-26T23:36:28.214191Z","iopub.status.idle":"2024-12-26T23:36:28.230565Z","shell.execute_reply.started":"2024-12-26T23:36:28.214129Z","shell.execute_reply":"2024-12-26T23:36:28.229328Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"   Shell weight  Height  Diameter  Whole weight  Sex_I  Sex_M\n0        0.3005   0.155     0.475        1.2380  False   True\n1        0.2750   0.160     0.460        0.9830  False   True\n2        0.2405   0.140     0.420        0.8395  False   True\n3        0.2350   0.145     0.490        0.8740  False   True\n4        0.1050   0.110     0.325        0.3580   True  False","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Shell weight</th>\n      <th>Height</th>\n      <th>Diameter</th>\n      <th>Whole weight</th>\n      <th>Sex_I</th>\n      <th>Sex_M</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.3005</td>\n      <td>0.155</td>\n      <td>0.475</td>\n      <td>1.2380</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.2750</td>\n      <td>0.160</td>\n      <td>0.460</td>\n      <td>0.9830</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.2405</td>\n      <td>0.140</td>\n      <td>0.420</td>\n      <td>0.8395</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.2350</td>\n      <td>0.145</td>\n      <td>0.490</td>\n      <td>0.8740</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.1050</td>\n      <td>0.110</td>\n      <td>0.325</td>\n      <td>0.3580</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":31},{"cell_type":"markdown","source":"An attempt at an ensemble model, based upon https://www.kaggle.com/code/kqyan1990/abalone-prediction-a-complete-notebook","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def root_mean_squared_log_error(y_true, y_pred):\n    return np.sqrt(mean_squared_log_error(y_true, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T23:36:32.221865Z","iopub.execute_input":"2024-12-26T23:36:32.222294Z","iopub.status.idle":"2024-12-26T23:36:32.227225Z","shell.execute_reply.started":"2024-12-26T23:36:32.222263Z","shell.execute_reply":"2024-12-26T23:36:32.225798Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"def evaluate_model(true, predicted):\n    # mae = mean_absolute_error(true, predicted)\n    # mse = mean_squared_error(true, predicted)\n    # rmse = np.sqrt(mean_squared_error(true, predicted))\n    r2_square = r2_score(true, predicted)\n    rmsle = root_mean_squared_log_error(true, predicted)\n    return r2_square,rmsle ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T23:36:34.380072Z","iopub.execute_input":"2024-12-26T23:36:34.380473Z","iopub.status.idle":"2024-12-26T23:36:34.385689Z","shell.execute_reply.started":"2024-12-26T23:36:34.380440Z","shell.execute_reply":"2024-12-26T23:36:34.384342Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso, ElasticNet,Perceptron, SGDClassifier\nfrom sklearn.svm import SVC, LinearSVC, SVR\nfrom sklearn.ensemble import  AdaBoostRegressor, RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nimport lightgbm as lgb\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_log_error\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T23:36:35.943956Z","iopub.execute_input":"2024-12-26T23:36:35.944375Z","iopub.status.idle":"2024-12-26T23:36:35.950494Z","shell.execute_reply.started":"2024-12-26T23:36:35.944338Z","shell.execute_reply":"2024-12-26T23:36:35.949210Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"# create a utility function to sort the dictionary by values aphabatically\ndef sort_dict(d):\n    return dict(sorted(d.items(), key=lambda x: x[0]))\n\n# Define a list of models for prediction\nmodels = {\n    \"Linear Regression\": LinearRegression(),\n    #\"Lasso\": Lasso(),\n    #\"SVR\": SVR(),\n    #\"K-Neighbors Regressor\": KNeighborsRegressor(),\n    #\"Decision Tree\": DecisionTreeRegressor(),\n    \"Random Forest Regressor\": RandomForestRegressor(),\n    #\"Gradient Boosting Regressor\": GradientBoostingRegressor(),\n    \"Extra Trees Regressor\": ExtraTreesRegressor(),\n    \"XGBRegressor\": XGBRegressor(), \n    \"CatBoosting Regressor\": CatBoostRegressor(verbose=False),\n    #\"AdaBoost Regressor\": AdaBoostRegressor(),\n    #\"LightGBM\": lgb.LGBMRegressor(),\n    'DummyRegressor': DummyRegressor(strategy='mean') # DummyRegressor is added for sanity check\n}\n\n# Sort the models\nmodels=sort_dict(models)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T23:36:39.073650Z","iopub.execute_input":"2024-12-26T23:36:39.074010Z","iopub.status.idle":"2024-12-26T23:36:39.083632Z","shell.execute_reply.started":"2024-12-26T23:36:39.073981Z","shell.execute_reply":"2024-12-26T23:36:39.082208Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# Define the parameters for the hyperparamter tuning of models by RandomizedSearchCV\nimport scipy.stats as stats\nparams={\n\n    # \"Decision Tree\":{\n    #       'criterion':['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n    #       # 'splitter':['best','random'],\n    #       # 'max_features':['sqrt','log2'],  \n    # },\n     \n     \"Random Forest Regressor\":{\n         # 'criterion':['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n         # 'max_features':['sqrt','log2',None],\n         'n_estimators': [8,16,32,64,128,256]\n     },\n\n                    \n    #  \"Gradient Boosting\":{\n    #       # 'loss':['squared_error', 'huber', 'absolute_error', 'quantile'],\n    #       'learning_rate':[.1,.01,.05,.001],\n    #       'subsample':[0.6,0.7,0.75,0.8,0.85,0.9],\n    #       # 'criterion':['squared_error', 'friedman_mse'],\n    #       # 'max_features':['auto','sqrt','log2'],\n    #       'n_estimators': [8,16,32,64,128,256]\n    #  },\n\n     \"Linear Regression\":{},\n\n     \"XGBRegressor\":{\n        'booster': ['gbtree','dart'],\n        'reg_alpha': stats.uniform(0, 1),\n        'learning_rate':stats.loguniform(1e-2, 1e-1),\n        'n_estimators': [8,16,32,64,128,256]\n     },\n\n     \"CatBoosting Regressor\":{\n         'depth': [6,8,10],\n          'learning_rate': stats.loguniform(1e-2, 1e-1),\n          'iterations': [30, 50, 100]\n     },\n\n    #  \"AdaBoost Regressor\":{\n    #       'learning_rate':[.1,.01,0.5,.001],\n    #       # 'loss':['linear','square','exponential'],\n    #       'n_estimators': [8,16,32,64,128,256]\n    #  },\n            \n     \"Extra Trees Regressor\":{\n          # 'criterion':['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n          # 'max_features':['sqrt','log2',None],\n          'n_estimators': [8,16,32,64,128,256]\n     },\n\n    #  \"K-Neighbors Regressor\":{\n    #     'n_neighbors': [3,5,7,9],\n    #     'weights': ['uniform','distance'],\n    #     'algorithm': ['auto','ball_tree','kd_tree','brute']\n    # },\n                \n    # \"Lasso\":{\n    #     'alpha': [0.1,0.5,1.0,1.5],\n    #     'selection': ['cyclic','random']\n    # },\n\n    # \"SVR\":{\n    #     'kernel': ['linear','poly','rbf','sigmoid'],\n    #     'C': [0.1,1,10,100,1000],\n    #     'gamma': ['scale','auto']\n    # },\n\n    \"DummyRegressor\":{}\n\n}\n\n# Sort the parameters\nparams = sort_dict(params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T23:36:43.074125Z","iopub.execute_input":"2024-12-26T23:36:43.074519Z","iopub.status.idle":"2024-12-26T23:36:43.084984Z","shell.execute_reply.started":"2024-12-26T23:36:43.074491Z","shell.execute_reply":"2024-12-26T23:36:43.083289Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"from sklearn.metrics import make_scorer\n\n# Define a custom cost function for the RandomizedSearchCV\ndef rmsle(y_true, y_pred):\n    rmsle= root_mean_squared_log_error(y_true, y_pred)\n    return rmsle","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T23:36:47.244033Z","iopub.execute_input":"2024-12-26T23:36:47.244444Z","iopub.status.idle":"2024-12-26T23:36:47.250303Z","shell.execute_reply.started":"2024-12-26T23:36:47.244411Z","shell.execute_reply":"2024-12-26T23:36:47.248573Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"from time import time\n# Define a function for initiating empty lists for storing the model parameters and the scores\ndef model_evaluation(models, params, X_train, y_train, X_test, y_test, kf):\n    model_list = []\n    r2_train_list =[]\n    r2_test_list = []\n    rmsle_train_list =[]\n    rmsle_test_list = []\n    time_list =[]\n    y_train_pred_list=[]\n    model_params = []\n\n    for i in range(len(list(models))):\n        try:\n            model = list(models.values())[i]\n            para=params[list(models.keys())[i]]\n            # implement a nested cross-validation\n            start=time()\n            RS = RandomizedSearchCV(model, para, n_iter=10, cv=kf, scoring=make_scorer(rmsle, greater_is_better=False),refit=True, n_jobs=-1, verbose=1)\n            RS.fit(X_train, y_train) # Train model\n            time_list.append(time()-start)\n            model.set_params(**RS.best_params_)\n            model.fit(X_train, y_train)\n\n            # Make predictions\n            y_train_pred = model.predict(X_train)\n            y_test_pred = model.predict(X_test)\n        \n            # Evaluate Train and Test dataset\n            r2_train, rmsle_train = evaluate_model(y_train, y_train_pred)\n\n            r2_test, rmsle_test = evaluate_model(y_test, y_test_pred)\n\n            # Append the results to the lists     \n            model_list.append(list(models.keys())[i])\n            r2_train_list.append(r2_train)\n            r2_test_list.append(r2_test)\n            rmsle_train_list.append(rmsle_train)\n            rmsle_test_list.append(rmsle_test)\n            y_train_pred_list.append(y_train_pred)\n            # Append the model parameters\n            model_params.append(RS.best_params_)\n\n            print('Model Success:',list(models.keys())[i], 'R2:', r2_test, 'RMSLE:', rmsle_test)\n            print('='*35)\n            print('\\n')\n\n            # Print the best hyperparameters\n            print('Best hyperparameters:',  **RS.best_params_)\n\n        # Raise exception if the model fails\n        except Exception as e:\n            print(list(models.keys())[i])\n            model_list.append(list(models.keys())[i])\n            r2_train_list.append(np.nan)\n            r2_test_list.append(np.nan)\n            rmsle_train_list.append(np.nan)\n            rmsle_test_list.append(np.nan)\n            print('Model failed:', e)\n            print('='*35)\n            print('\\n')\n            continue\n\n    return model_list, r2_train_list, r2_test_list, rmsle_train_list, rmsle_test_list, time_list, y_train_pred_list,model_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T23:48:11.025066Z","iopub.execute_input":"2024-12-26T23:48:11.025505Z","iopub.status.idle":"2024-12-26T23:48:11.037182Z","shell.execute_reply.started":"2024-12-26T23:48:11.025471Z","shell.execute_reply":"2024-12-26T23:48:11.035891Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nSEED=0\n# Setup the KFold\nNFOLDS = 5 # set folds for out-of-fold prediction\nkf = KFold(n_splits= NFOLDS,shuffle=True, random_state=SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T23:37:18.106356Z","iopub.execute_input":"2024-12-26T23:37:18.106708Z","iopub.status.idle":"2024-12-26T23:37:18.112297Z","shell.execute_reply.started":"2024-12-26T23:37:18.106680Z","shell.execute_reply":"2024-12-26T23:37:18.110650Z"}},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":"Catboost failure: https://stackoverflow.com/questions/68950922/unable-to-tune-hyperparameters-for-catboostregressor","metadata":{}},{"cell_type":"code","source":"# Scores with feature engineered data\nmodel_list, r2_train_list, r2_test_list, rmsle_train_list, rmsle_test_list, time_list,y_train_pred_list, model_params= model_evaluation(models, params, X_train, y_train, X_test, y_test, kf)\n\n# Display the scores\npd.DataFrame(list(zip(model_list,time_list, r2_train_list,r2_test_list,rmsle_train_list,rmsle_test_list)), columns=['Model Name','Compute time', 'r2_Score_Train','r2_Score_test','RMSLE_Score_Train','RMSLE_Score_Test']).sort_values(by=[\"RMSLE_Score_Train\"],ascending=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T23:48:24.982146Z","iopub.execute_input":"2024-12-26T23:48:24.982540Z"}},"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 10 candidates, totalling 50 fits\nCatBoosting Regressor\nModel failed: You can't change params of fitted model.\n===================================\n\n\nFitting 5 folds for each of 1 candidates, totalling 5 fits\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 1 is smaller than n_iter=10. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Model Success: DummyRegressor R2: -2.4498836381781075e-07 RMSLE: 0.29094884304084784\n===================================\n\n\nBest hyperparameters:\nFitting 5 folds for each of 6 candidates, totalling 30 fits\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 6 is smaller than n_iter=10. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Model Success: Extra Trees Regressor R2: 0.5387285402401927 RMSLE: 0.1779692191978259\n===================================\n\n\nExtra Trees Regressor\nModel failed: 'n_estimators' is an invalid keyword argument for print()\n===================================\n\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 1 is smaller than n_iter=10. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Fitting 5 folds for each of 1 candidates, totalling 5 fits\nModel Success: Linear Regression R2: 0.546040614328724 RMSLE: 0.17421514122468837\n===================================\n\n\nBest hyperparameters:\nFitting 5 folds for each of 6 candidates, totalling 30 fits\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 6 is smaller than n_iter=10. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"The XGBRegressor takes far longer to get slightly better test results.  Note that some of the other methods seem to overfit and get better training results.","metadata":{}},{"cell_type":"code","source":"# Plot the y_train and y_train_pred \nf_cols=3\nf_rows = np.ceil(len(list(models))/f_cols).astype(int)\nfig, ax = plt.subplots(f_rows, f_cols, figsize=(15, 10))\nfor i in range(len(y_train_pred_list)):\n    plt.tight_layout()\n    heatmap, xedges, yedges = np.histogram2d(x=y_train, y=y_train_pred_list[i].tolist(), bins=100)\n    cax=ax[i//f_cols, i%f_cols].imshow(heatmap.T, origin='lower', cmap='viridis', aspect='auto')\n    ax[i//f_cols, i%f_cols].set_xlabel('True Values')\n    ax[i//f_cols, i%f_cols].set_ylabel('Predictions')\n    ax[i//f_cols, i%f_cols].grid(False) \n    fig.colorbar(cax, ax=ax[i//f_cols, i%f_cols], label='Density')\n    ax[i//f_cols, i%f_cols].set_title(list(models.keys())[i])\n","metadata":{"trusted":true,"execution":{"execution_failed":"2024-12-26T23:32:41.729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot the RMSLE score of the models and sort them in descending order\nplt.figure(figsize=(15, 8))\nsns.barplot(x=rmsle_train_list, y=model_list)\nplt.title('Rmsle Score of the models')\nplt.xlabel('Rmsle Score')\nplt.ylabel('Models')\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2024-12-26T23:32:41.729Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now to try an ensemble...\n\nBased off of this example: https://www.kaggle.com/code/arthurtok/introduction-to-ensembling-stacking-in-python","metadata":{}},{"cell_type":"code","source":"# Define a function to obtain the out of fold predictions\ndef get_oof_predictions(model, X_train, y_train, X_test, kf):\n    \n    # Initialize the out of fold predictions\n    oof_predictions = np.zeros((X_train.shape[0],))\n    # Initialize the test predictions\n    test_predictions = np.zeros((X_test.shape[0],))\n\n    # Loop through the training and validation sets\n    for train_index, val_index in kf.split(X_train):\n        # Split the training and validation sets\n        X_tr, X_val = X_train[train_index], X_train[val_index]\n        y_tr, y_val = y_train[train_index], y_train[val_index]\n        # Fit the model\n        model.fit(X_tr, y_tr) # is this necessary?\n        # Make predictions\n        oof_predictions[val_index] = model.predict(X_val)\n        test_predictions += model.predict(X_test)\n\n    # Return the out of fold predictions and the test predictions\n    return oof_predictions, test_predictions / kf.get_n_splits() # Average the test predictions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize a dataframe of out of fold predictions for the models\noof_predictions_df = pd.DataFrame()\n# Initialize a dataframe of test predictions for the models\ntest_predictions_df = pd.DataFrame()\n\n# Get the out of fold predictions from the models\nfor i in range(len(list(models))):\n    model = list(models.values())[i]\n    oof_predictions, test_predictions = get_oof_predictions(model, X_train, y_train, X_test, kf)\n    oof_predictions_df[list(models.keys())[i]] = oof_predictions\n    test_predictions_df[list(models.keys())[i]] = test_predictions\n\n# Display the out of fold predictions\noof_predictions_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot the out of fold predictions in the same figure\nplt.figure(figsize=(10, 6))\nfor i in range(len(list(models))):\n    sns.scatterplot(x=y_train, y=oof_predictions_df[list(models.keys())[i]], label=list(models.keys())[i])\n# Plot the identity line\nplt.plot([0, 30], [0, 30], color='black', linestyle='--')\nplt.legend()\nplt.xlabel('True Values')\nplt.ylabel('Predictions')\nplt.title('Out of Fold Predictions')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract the feature importances from the models\nfeature_importances = pd.DataFrame()\nfor i in range(len(list(models))):\n    model = list(models.values())[i]\n    try:\n        feature_importances[list(models.keys())[i]] = model.feature_importances_\n    except AttributeError:\n        feature_importances[list(models.keys())[i]] = np.nan\n\n# Display the feature importances\nfeature_importances","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plot the correlation matrix of the out of fold predictions\nplt.figure(figsize=(15, 10))\nsns.heatmap(oof_predictions_df_selected.corr(), annot=True, fmt=\".2f\", linewidths=0.5, cmap='coolwarm', square=True, cbar_kws={\"shrink\": 0.5})\nplt.title('Correlation Plot of the Out of Fold Predictions', fontsize=22)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a dataframe to store the OOF predictions from selected models\noof_predictions_df_selected = pd.DataFrame()\noof_predictions_df_selected = oof_predictions_df[['CatBoosting Regressor','XGBRegressor','Random Forest Regressor','Extra Trees Regressor','Linear Regression']].copy()\n\n# Create a dataframe to store the test predictions from selected models\ntest_predictions_df_selected = pd.DataFrame()\ntest_predictions_df_selected = test_predictions_df[['CatBoosting Regressor','XGBRegressor','Random Forest Regressor','Extra Trees Regressor','Linear Regression']].copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Not sure why the source I am working from using the training results.  These test results are more likely to be useful...\n\nReally intesting to compare the results to see which methods generalize better.","metadata":{}},{"cell_type":"code","source":"# Plot the RMSLE score of the models and sort them in descending order\nplt.figure(figsize=(15, 8))\nsns.barplot(x=rmsle_test_list, y=model_list)\nplt.title('Rmsle Score of the models')\nplt.xlabel('Rmsle Score')\nplt.ylabel('Models')\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2024-12-26T23:32:41.730Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## The followings are the steps to add back the selected features to the original features and fit the meta-model (optional)\n# # Concanate the oof_predictions_df_selected with the original features\n# X_train_meta = np.concatenate([X_train, oof_predictions_df_selected], axis=1)\n# # Concanate the test_predictions_df_selected with the original features\n# X_test_meta = np.concatenate([X_test, test_predictions_df_selected], axis=1)\n\n# Setup a pipeline for the meta-model\nmlp=make_pipeline(StandardScaler(),MLPRegressor(hidden_layer_sizes=(100, 100), tol=1e-2, max_iter=500, random_state=0))\n\n# Implement a randomized search for the meta-model\nparams_mlp = {\n    'mlpregressor__hidden_layer_sizes':[(100, 100), (200, 200), (300, 300)],\n    'mlpregressor__alpha': stats.uniform(0, 1),\n    'mlpregressor__learning_rate_init': stats.loguniform(1e-3, 1e-1),\n    'mlpregressor__max_iter': [100, 200, 300],\n}\n\n# Setup the RandomizedSearchCV\nRS_mlp = RandomizedSearchCV(mlp, params_mlp, n_iter=10, cv=kf, scoring=make_scorer(rmsle, greater_is_better=False),refit=True, n_jobs=-1, verbose=1)\n\n# Fit the RandomizedSearchCV\nRS_mlp.fit(oof_predictions_df_selected, y_train) # if original features are added, then the oof_predictions_df_selected is replaced by X_train_meta","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Produce the final predictions from the meta-model\nFinal_predictions=RS_mlp.predict(test_predictions_df_selected) # Or X_test_meta if original features are used\n# Evaluate the final model\nfinal_score=evaluate_model(y_test, Final_predictions)\nprint(f\"RMSLE Score of the Stacking Regressor on the testing set: {final_score[1]}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate the correlation of the final predictions and the real target\ncorrelation = np.corrcoef(y_test, Final_predictions)[0, 1]\ncorrelation","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Build out the tree on the training data\nThe max depth and number of estimators is taken from a RandomizedSearchCV which is not done here, as it take a while to run","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n# Initialize the model\nrf_model = RandomForestRegressor(max_depth=16, n_estimators=297)\n\n# Train the model\nrf_model.fit(X_train, y_train)  # No scaling needed for tree-based models\n\n# Make predictions on the test set\ny_pred_rf = rf_model.predict(X_test)\n\n# Evaluate the model\nprint(\"Random Forest Metrics:\")\nprint(f\"R^2 Score: {r2_score(y_test, y_pred_rf):.3f}\")\nprint(f\"Mean Absolute Error (MAE): {mean_absolute_error(y_test, y_pred_rf):.3f}\")\nprint(f\"Root Mean Squared Error (RMSE): {mean_squared_error(y_test, y_pred_rf, squared=False):.3f}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2024-12-26T23:32:41.730Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Use the model created to predict final results","metadata":{}},{"cell_type":"code","source":"submission_pred = rf_model.predict(submission_selected)","metadata":{"trusted":true,"execution":{"execution_failed":"2024-12-26T23:32:41.730Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Combine output with ids in the proper format","metadata":{}},{"cell_type":"code","source":"id_df = submission_df.id\npred_df = pd.DataFrame(submission_pred, columns=['Rings'])\n\nfinal_df = pd.concat([id_df, pred_df], axis=1)\n\nfinal_df.head()","metadata":{"trusted":true,"execution":{"execution_failed":"2024-12-26T23:32:41.730Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_df.to_csv('abalone-submission-01.csv', index=False)","metadata":{"trusted":true,"execution":{"execution_failed":"2024-12-26T23:32:41.730Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Submitting attempts\n\nLike so many things in Kaggle there is a lot of outdated information, or posts which attempt to be helpful but skip by\ncrucial info.  Attempting to figure out how to submit this info took way too long.  \n\nOne key: When searching on Google ensure that you only look for results in the past year. Otherwise outdated results for Kaggle predominate.\n\nThis site was of some use: https://www.kaggle.com/discussions/questions-and-answers/518559\n\nBut the key step was to go back to the submissions page: https://www.kaggle.com/competitions/playground-series-s4e4/submissions\nand notice that a Late Submission button exist. This is a crap UI as the 'button' does not have an outline so is almost undetectable in dark mode.\n\nOnce I found that button my submission quickly worked.  Now to figure out how good the solution is.  Kaggle shows my results, but gives no\ncontext as to how that compares to anything else.\n\nMy initial results: private score 0.16294, public score 0.16208\n\nThe leaderboard shows that the best results are around 0.14374\nhttps://www.kaggle.com/competitions/playground-series-s4e4/leaderboard\n\nThe differences into the hundreds of positions are tiny--around 0.00126\n\nI assume the leaderboard only reflects submissions made before the deadline.","metadata":{}}]}